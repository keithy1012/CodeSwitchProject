[  {
    "id": 1,
    "utterance": "Li: \"昨天的 nightly build 挂了，你有看到吗?\"\nSarah: \"Yeah, log 里面说是一个 dependency 没有 properly 安装.\"\nLi: \"我试过 rerun build, 还是 fail. 可能需要 pin version.\"\nSarah: \"好，我开个 PR 去 lock version, 然后我们再 merge 一次.\"\nLi: \"OK，merge 完之后我会再跑一遍 CI.\"\nSarah: \"等确认 pass 了，再准备 deploy 到 staging.\"",
    "source": "LLM",
    "generation_strategy": ["Persona-based", "Few-shot"],
    "model_name": "GPT-4",
    "prompt_text": "You are an expert dataset generator for bilingual English–Mandarin code-switched technical dialogues. Act as a frontend engineer from Shanghai now working in San Francisco. You naturally mix English technical terms into Mandarin sentences, and sometimes switch whole phrases to English when it feels natural. Keep the tone pragmatic, casual, like real engineers chatting.\nFew-shot examples:\nExample 1:\nA: \"Can you push the latest styles to staging?\"\nB: \"可以，我现在就 push. 不过 header 在 mobile 上看起来有点 weird.\"\nExample 2:\nA: \"There's a bug in the login flow after the OAuth change.\"\nB: \"看一下 redirect URI, scope 好像没有 properly 设置.\"\nExample 3:\nA: \"We need to ship before Friday's deadline.\"\nB: \"好，先 prioritize login 和 checkout, 其他功能可以等下个 sprint.\"\nNow: produce a dialogue between two engineers (Li and Sarah). Each dialogue should be 5–7 turns.\nRequirements:\nUse **English-Mandarin code-switching** at the **phrase level** (switch after a technical word or phrase).\nMake sure at least one of the following trigger words appears in each dialogue: **deploy, build, bug, deadline, merge**.\nTarget ~35–50% Mandarin tokens per dialogue.\nUse natural engineering register (issue, PR, deploy, QA, staging, workaround).\nKeep turns short-to-medium.\nOutput only the dialogues. Do not repeat the few-shot examples."
}
,{
    "id": 2,
    "utterance": "Sarah: \"这个 checkout flow 有个 bug, 用户在 Safari 上卡住.\"\nLi: \"我 reproduce 了一下，好像是 localStorage 没 properly update.\"\nSarah: \"那我们要不要加个小 workaround?\"\nLi: \"可以，写个 fallback logic. 我晚点 push 到 branch.\"\nSarah: \"行，等你 push 完我来 review PR.\"\nLi: \"Review 后尽快 merge，不然 QA 没法继续测.\"",
    "source": "LLM",
    "generation_strategy": ["Persona-based", "Few-shot"],
    "model_name": "GPT-4",
    "prompt_text": "You are an expert dataset generator for bilingual English–Mandarin code-switched technical dialogues. Act as a frontend engineer from Shanghai now working in San Francisco. You naturally mix English technical terms into Mandarin sentences, and sometimes switch whole phrases to English when it feels natural. Keep the tone pragmatic, casual, like real engineers chatting.\nFew-shot examples:\nExample 1:\nA: \"Can you push the latest styles to staging?\"\nB: \"可以，我现在就 push. 不过 header 在 mobile 上看起来有点 weird.\"\nExample 2:\nA: \"There's a bug in the login flow after the OAuth change.\"\nB: \"看一下 redirect URI, scope 好像没有 properly 设置.\"\nExample 3:\nA: \"We need to ship before Friday's deadline.\"\nB: \"好，先 prioritize login 和 checkout, 其他功能可以等下个 sprint.\"\nNow: produce a dialogue between two engineers (Li and Sarah). Each dialogue should be 5–7 turns.\nRequirements:\nUse **English-Mandarin code-switching** at the **phrase level** (switch after a technical word or phrase).\nMake sure at least one of the following trigger words appears in each dialogue: **deploy, build, bug, deadline, merge**.\nTarget ~35–50% Mandarin tokens per dialogue.\nUse natural engineering register (issue, PR, deploy, QA, staging, workaround).\nKeep turns short-to-medium.\nOutput only the dialogues. Do not repeat the few-shot examples."
}
, {
    "id": 3,
    "utterance": "Li: \"我们这周的 deadline 快到了，dashboard 还没 ready.\"\nSarah: \"对，前端部分我还在 refactor chart component.\"\nLi: \"要不要先 cut 一个简单版？deploy 上去给 PM 看.\"\nSarah: \"好，我可以先把 basic filters 留着，其他 advance 的先砍掉.\"\nLi: \"这样至少能 meet deadline，剩下的功能下个 sprint 加.\"\nSarah: \"Sounds good，我马上 push 到 staging branch.\"",
    "source": "LLM",
    "generation_strategy": ["Persona-based", "Few-shot"],
    "model_name": "GPT-4",
    "prompt_text": "You are an expert dataset generator for bilingual English–Mandarin code-switched technical dialogues. Act as a frontend engineer from Shanghai now working in San Francisco. You naturally mix English technical terms into Mandarin sentences, and sometimes switch whole phrases to English when it feels natural. Keep the tone pragmatic, casual, like real engineers chatting.\nFew-shot examples:\nExample 1:\nA: \"Can you push the latest styles to staging?\"\nB: \"可以，我现在就 push. 不过 header 在 mobile 上看起来有点 weird.\"\nExample 2:\nA: \"There's a bug in the login flow after the OAuth change.\"\nB: \"看一下 redirect URI, scope 好像没有 properly 设置.\"\nExample 3:\nA: \"We need to ship before Friday's deadline.\"\nB: \"好，先 prioritize login 和 checkout, 其他功能可以等下个 sprint.\"\nNow: produce a dialogue between two engineers (Li and Sarah). Each dialogue should be 5–7 turns.\nRequirements:\nUse **English-Mandarin code-switching** at the **phrase level** (switch after a technical word or phrase).\nMake sure at least one of the following trigger words appears in each dialogue: **deploy, build, bug, deadline, merge**.\nTarget ~35–50% Mandarin tokens per dialogue.\nUse natural engineering register (issue, PR, deploy, QA, staging, workaround).\nKeep turns short-to-medium.\nOutput only the dialogues. Do not repeat the few-shot examples."
}
,{
    "id": 4,
    "utterance": "Sarah: \"昨天的 merge 把 config 搞坏了，production deploy fail 了.\"\nLi: \"啊？我以为 QA 都 pass 了才 merge 的.\"\nSarah: \"QA pass 了，但 prod config file 不一样.\"\nLi: \"那我们要 hotfix 吗？\"\nSarah: \"Yeah，先开个小 patch branch，改 config 再 redeploy.\"\nLi: \"我来处理，你帮忙 double-check 环境变量.\"\nSarah: \"OK, 我会在 Slack 上 confirm 一下.\"",
    "source": "LLM",
    "generation_strategy": ["Persona-based", "Few-shot"],
    "model_name": "GPT-4",
    "prompt_text": "You are an expert dataset generator for bilingual English–Mandarin code-switched technical dialogues. Act as a frontend engineer from Shanghai now working in San Francisco. You naturally mix English technical terms into Mandarin sentences, and sometimes switch whole phrases to English when it feels natural. Keep the tone pragmatic, casual, like real engineers chatting.\nFew-shot examples:\nExample 1:\nA: \"Can you push the latest styles to staging?\"\nB: \"可以，我现在就 push. 不过 header 在 mobile 上看起来有点 weird.\"\nExample 2:\nA: \"There's a bug in the login flow after the OAuth change.\"\nB: \"看一下 redirect URI, scope 好像没有 properly 设置.\"\nExample 3:\nA: \"We need to ship before Friday's deadline.\"\nB: \"好，先 prioritize login 和 checkout, 其他功能可以等下个 sprint.\"\nNow: produce a dialogue between two engineers (Li and Sarah). Each dialogue should be 5–7 turns.\nRequirements:\nUse **English-Mandarin code-switching** at the **phrase level** (switch after a technical word or phrase).\nMake sure at least one of the following trigger words appears in each dialogue: **deploy, build, bug, deadline, merge**.\nTarget ~35–50% Mandarin tokens per dialogue.\nUse natural engineering register (issue, PR, deploy, QA, staging, workaround).\nKeep turns short-to-medium.\nOutput only the dialogues. Do not repeat the few-shot examples."
}
,{
    "id": 5,
    "utterance": "Li: \"Login 页面 layout 在 Android 上 broken, user report 一个 bug.\"\nSarah: \"我看了下，是 CSS flex 在 old Chrome version 不支持.\"\nLi: \"那要不要写个 polyfill, 或者直接改成 grid?\"\nSarah: \"Grid 更 stable, 我可以马上 build 一版.\"\nLi: \"Build 完 push 到 feature branch, 我来 test.\"\nSarah: \"好，等你确认没问题，我们 merge 后 deploy staging.\"\nLi: \"OK，赶紧搞定，不然 QA 会卡住.\"",
    "source": "LLM",
    "generation_strategy": ["Persona-based", "Few-shot"],
    "model_name": "GPT-4",
    "prompt_text": "You are an expert dataset generator for bilingual English–Mandarin code-switched technical dialogues. Act as a frontend engineer from Shanghai now working in San Francisco. You naturally mix English technical terms into Mandarin sentences, and sometimes switch whole phrases to English when it feels natural. Keep the tone pragmatic, casual, like real engineers chatting.\nFew-shot examples:\nExample 1:\nA: \"Can you push the latest styles to staging?\"\nB: \"可以，我现在就 push. 不过 header 在 mobile 上看起来有点 weird.\"\nExample 2:\nA: \"There's a bug in the login flow after the OAuth change.\"\nB: \"看一下 redirect URI, scope 好像没有 properly 设置.\"\nExample 3:\nA: \"We need to ship before Friday's deadline.\"\nB: \"好，先 prioritize login 和 checkout, 其他功能可以等下个 sprint.\"\nNow: produce a dialogue between two engineers (Li and Sarah). Each dialogue should be 5–7 turns.\nRequirements:\nUse **English-Mandarin code-switching** at the **phrase level** (switch after a technical word or phrase).\nMake sure at least one of the following trigger words appears in each dialogue: **deploy, build, bug, deadline, merge**.\nTarget ~35–50% Mandarin tokens per dialogue.\nUse natural engineering register (issue, PR, deploy, QA, staging, workaround).\nKeep turns short-to-medium.\nOutput only the dialogues. Do not repeat the few-shot examples."
}
,   {
    "id": 6,
    "utterance": "Chen: \"We saw latency spikes after 11 PM.\"\nJohn: \"Yes, cache miss rate went up sharply.\"\nChen: \"这是昨天晚上 deploy 引起的.\"\nJohn: \"Right, we should rollback 那个 microservice.\"\nChen: \"好的，我准备 rollback plan 并通知 infra.\"\nJohn: \"I'll add monitoring 指标 for cache hits.\"",
    "source": "LLM",
    "generation_strategy": ["Persona-based", "Chain of Thought"],
    "model_name": "GPT-4",
    "prompt_text": "You are a senior bilingual engineer (Chen) from Beijing working on distributed systems in Seattle. Write English-Mandarin code-switched dialogues reflecting real engineering reasoning. Chen is precise and professional but mixes Mandarin naturally in work chats."
  }, {
    "id": 7,
    "utterance": "Chen: \"The timeout logic is fine, but circuit breaker 配置太 aggressive.\"\nJohn: \"It trips after two failures, 然后导致级联错误.\"\nChen: \"我们应该加 threshold 并且 implement exponential backoff.\"\nJohn: \"Right — client retries 现在是 sync, 要改成 async.\"\nChen: \"我来 patch client lib 并更新 doc.\"\nJohn: \"Perfect, 那我们就能 benchmark 修复效果.\"",
    "source": "LLM",
    "generation_strategy": ["Persona-based", "Chain of Thought"],
    "model_name": "GPT-4",
    "prompt_text": "You are a senior bilingual engineer (Chen) from Beijing working on distributed systems in Seattle. Write English-Mandarin code-switched dialogues reflecting real engineering reasoning. Chen is precise and professional but mixes Mandarin naturally in work chats."
  },
    {
    "id": 8,
    "utterance": "Chen: \"This API contract is incompatible, 对吧?\"\nJohn: \"对，他们忘了加 version.\"\nChen: \"We need a migration path, 是不是?\"\nJohn: \"Exactly — deprecate 老接口然后加个 shim.\"\nChen: \"我写 migration plan, maintain dual writes 两个 sprint.\"\nJohn: \"我联系 SDK team 确保他们 clients 不会 break.\"",
    "source": "LLM",
    "generation_strategy": ["Persona-based", "Chain of Thought"],
    "model_name": "GPT-4",
    "prompt_text": "You are a senior bilingual engineer (Chen) from Beijing working on distributed systems in Seattle. Write English-Mandarin code-switched dialogues reflecting real engineering reasoning. Chen is precise and professional but mixes Mandarin naturally in work chats."
  },
    {
    "id": 9,
    "utterance": "Chen: \"There's a payment bug after tokenization.\"\nJohn: \"Token service fails 在某些卡类型.\"\nChen: \"先 hotfix, long-term 再重构.\"\nJohn: \"我加 logging 并且启用 A/B test 来验证.\"\nChen: \"Staging pass 后 we deploy to prod.\"\nJohn: \"好，我也准备 rollback steps 以防万一.\"",
    "source": "LLM",
    "generation_strategy": ["Persona-based", "Chain of Thought"],
    "model_name": "GPT-4",
    "prompt_text": "You are a senior bilingual engineer (Chen) from Beijing working on distributed systems in Seattle. Write English-Mandarin code-switched dialogues reflecting real engineering reasoning. Chen is precise and professional but mixes Mandarin naturally in work chats."
  },
  {
    "id": 10,
    "utterance": "A: \"I'll deploy 新 feature branch this afternoon.\"\nB: \"好，别忘了 to run smoke tests.\"\nA: \"If pass, 我就 tag release 并通知 QA.\"\nB: \"Perfect, 然后我们就 deploy.\"",
    "source": "LLM",
    "generation_strategy": ["Few-shot", "Chain-of-Thought"],
    "model_name": "GPT-4",
    "prompt_text": "Generate English-Mandarin code-switched dialogues showing micro-level switching patterns. Few-shot examples:\nTemplate 1 (verb-boundary):\nA: \"I'm going to deploy 新分支 right now.\"\nB: \"好，记得 to bump version number.\"\nTemplate 2 (idiom + switch):\nA: \"That fix is a 临时方案 but it works.\"\nB: \"是的，不过我们得做 proper refactor.\"\nTemplate 3 (phrasal-trigger):\nA: \"If cache misses, call fallback.\"\nB: \"如果这样, we'll log event 并且 alert team.\"\nPattern: verb-boundary."
  },
  {
    "id": 11,
    "utterance": "A: \"这个 hotfix 只是权宜之计, but it avoids outage.\"\nB: \"是的，下个 sprint 我们必须 refactor 那部分.\"\nA: \"Agree, uptime first, refactor later.\"\nB: \"我加一个 ticket 把 refactor 细节写清楚.\"",
    "source": "LLM",
    "generation_strategy": ["Few-shot", "Chain-of-Thought"],
    "model_name": "GPT-4",
    "prompt_text": "Generate English-Mandarin code-switched dialogues showing micro-level switching patterns. Few-shot examples:\nTemplate 1 (verb-boundary):\nA: \"I'm going to deploy 新分支 right now.\"\nB: \"好，记得 to bump version number.\"\nTemplate 2 (idiom + switch):\nA: \"That fix is a 临时方案 but it works.\"\nB: \"是的，不过我们得做 proper refactor.\"\nTemplate 3 (phrasal-trigger):\nA: \"If cache misses, call fallback.\"\nB: \"如果这样, we'll log event 并且 alert team.\"\nPattern: idiom + switch."
  },
  {
    "id": 12,
    "utterance": "A: \"If cache misses, fallback kicks in.\"\nB: \"如果 fallback 触发, increase timeout 然后 log event.\"\nA: \"Add metrics 来衡量 hit rate.\"\nB: \"有了数据我们就能调整 policy.\"",
    "source": "LLM",
    "generation_strategy": ["Few-shot", "Chain-of-Thought"],
    "model_name": "GPT-4",
    "prompt_text": "Generate English-Mandarin code-switched dialogues showing micro-level switching patterns. Few-shot examples:\nTemplate 1 (verb-boundary):\nA: \"I'm going to deploy 新分支 right now.\"\nB: \"好，记得 to bump version number.\"\nTemplate 2 (idiom + switch):\nA: \"That fix is a 临时方案 but it works.\"\nB: \"是的，不过我们得做 proper refactor.\"\nTemplate 3 (phrasal-trigger):\nA: \"If cache misses, call fallback.\"\nB: \"如果这样, we'll log event 并且 alert team.\"\nPattern: phrasal-trigger."
  },
  {
    "id": 13,
    "utterance": "A: \"Checkout flow 有个 bug when address fails.\"\nB: \"对，validator 不支持本地字符.\"\nA: \"I'll add normalization 然后跑 tests.\"\nB: \"我补 QA cases 来避免 regression.\"",
    "source": "LLM",
    "generation_strategy": ["Few-shot", "Chain-of-Thought"],
    "model_name": "GPT-4",
    "prompt_text": "Generate English-Mandarin code-switched dialogues showing micro-level switching patterns. Few-shot examples:\nTemplate 1 (verb-boundary):\nA: \"I'm going to deploy 新分支 right now.\"\nB: \"好，记得 to bump version number.\"\nTemplate 2 (idiom + switch):\nA: \"That fix is a 临时方案 but it works.\"\nB: \"是的，不过我们得做 proper refactor.\"\nTemplate 3 (phrasal-trigger):\nA: \"If cache misses, call fallback.\"\nB: \"如果这样, we'll log event 并且 alert team.\"\nPattern: phrasal-trigger."
  },
  {
    "id": 14,
    "utterance": "A: \"我要 bump version 然后 publish.\"\nB: \"好，update changelog 在 release 前.\"\nA: \"Ok, publish 后我会开 PR.\"\nB: \"Great, 那我们可以 deploy.\"",
    "source": "LLM",
    "generation_strategy": ["Few-shot", "Chain-of-Thought"],
    "model_name": "GPT-4",
    "prompt_text": "Generate English-Mandarin code-switched dialogues showing micro-level switching patterns. Few-shot examples:\nTemplate 1 (verb-boundary):\nA: \"I'm going to deploy 新分支 right now.\"\nB: \"好，记得 to bump version number.\"\nTemplate 2 (idiom + switch):\nA: \"That fix is a 临时方案 but it works.\"\nB: \"是的，不过我们得做 proper refactor.\"\nTemplate 3 (phrasal-trigger):\nA: \"If cache misses, call fallback.\"\nB: \"如果这样, we'll log event 并且 alert team.\"\nPattern: verb-boundary."
  },
  {
    "id": 15,
    "utterance": "A: \"Merge 完成了, 可是 staging 有个 bug.\"\nB: \"那就先 revert, 然后开新 branch 修复.\"\nA: \"好的，我晚上写 patch.\"\nB: \"别忘了加 unit tests.\"",
    "source": "LLM",
    "generation_strategy": ["Few-shot", "Chain-of-Thought"],
    "model_name": "GPT-4",
    "prompt_text": "Generate English-Mandarin code-switched dialogues showing micro-level switching patterns. Few-shot examples:\nTemplate 1 (verb-boundary):\nA: \"I'm going to deploy 新分支 right now.\"\nB: \"好，记得 to bump version number.\"\nTemplate 2 (idiom + switch):\nA: \"That fix is a 临时方案 but it works.\"\nB: \"是的，不过我们得做 proper refactor.\"\nTemplate 3 (phrasal-trigger):\nA: \"If cache misses, call fallback.\"\nB: \"如果这样, we'll log event 并且 alert team.\"\nPattern: idiom + switch."
  },
  {
    "id": 16,
    "utterance": "Li: \"昨天 build 又 fail 了, 可能是新加的 dependency conflict.\"\nSarah: \"对，我看 log, 是版本 mismatch.\"\nLi: \"要不要先 revert 然后再 pin version?\"\nSarah: \"好，我开 PR 修复依赖，然后你再 merge.\"\nLi: \"Merge 后我跑一次 CI，确保 staging pass.\"\nSarah: \"OK, pass 后我们就准备 deploy.\"",
    "source": "LLM",
    "generation_strategy": ["Persona-based", "Few-shot"],
    "model_name": "GPT-4",
    "prompt_text": "You are an expert dataset generator for bilingual English–Mandarin code-switched technical dialogues. Act as a frontend engineer from Shanghai now working in San Francisco. You naturally mix English technical terms into Mandarin sentences, and sometimes switch whole phrases to English when it feels natural. Keep the tone pragmatic, casual, like real engineers chatting.\nFew-shot examples:\nExample 1:\nA: \"Can you push the latest styles to staging?\"\nB: \"可以，我现在就 push. 不过 header 在 mobile 上看起来有点 weird.\"\nExample 2:\nA: \"There's a bug in the login flow after the OAuth change.\"\nB: \"看一下 redirect URI, scope 好像没有 properly 设置.\"\nExample 3:\nA: \"We need to ship before Friday's deadline.\"\nB: \"好，先 prioritize login 和 checkout, 其他功能可以等下个 sprint.\"\nNow: produce a dialogue between two engineers (Li and Sarah). Each dialogue should be 5–7 turns.\nRequirements:\nUse **English-Mandarin code-switching** at the **phrase level** (switch after a technical word or phrase).\nMake sure at least one of the following trigger words appears in each dialogue: **deploy, build, bug, deadline, merge**.\nTarget ~35–50% Mandarin tokens per dialogue.\nUse natural engineering register (issue, PR, deploy, QA, staging, workaround).\nKeep turns short-to-medium.\nOutput only the dialogues. Do not repeat the few-shot examples."
  },
  {
    "id": 17,
    "utterance": "Sarah: \"这个 checkout 页面在 Safari 上有个 bug.\"\nLi: \"我看了，是 form validation 没 properly trigger.\"\nSarah: \"那我们先加个 fallback?\"\nLi: \"可以，写个 JS snippet 来 handle edge case.\"\nSarah: \"Snippet push 到 branch 后我来 review PR.\"\nLi: \"Review 后 merge, QA 就能继续测试.\"",
    "source": "LLM",
    "generation_strategy": ["Persona-based", "Few-shot"],
    "model_name": "GPT-4",
    "prompt_text": "You are an expert dataset generator for bilingual English–Mandarin code-switched technical dialogues. Act as a frontend engineer from Shanghai now working in San Francisco. You naturally mix English technical terms into Mandarin sentences, and sometimes switch whole phrases to English when it feels natural. Keep the tone pragmatic, casual, like real engineers chatting.\nFew-shot examples:\nExample 1:\nA: \"Can you push the latest styles to staging?\"\nB: \"可以，我现在就 push. 不过 header 在 mobile 上看起来有点 weird.\"\nExample 2:\nA: \"There's a bug in the login flow after the OAuth change.\"\nB: \"看一下 redirect URI, scope 好像没有 properly 设置.\"\nExample 3:\nA: \"We need to ship before Friday's deadline.\"\nB: \"好，先 prioritize login 和 checkout, 其他功能可以等下个 sprint.\"\nNow: produce a dialogue between two engineers (Li and Sarah). Each dialogue should be 5–7 turns.\nRequirements:\nUse **English-Mandarin code-switching** at the **phrase level** (switch after a technical word or phrase).\nMake sure at least one of the following trigger words appears in each dialogue: **deploy, build, bug, deadline, merge**.\nTarget ~35–50% Mandarin tokens per dialogue.\nUse natural engineering register (issue, PR, deploy, QA, staging, workaround).\nKeep turns short-to-medium.\nOutput only the dialogues. Do not repeat the few-shot examples."
  },
  {
    "id": 18,
    "utterance": "Li: \"我们这周 deadline 快到了, dashboard feature 还没完成.\"\nSarah: \"前端 chart 组件我还在 refactor.\"\nLi: \"要不要 cut 个 MVP 先 deploy 给 PM 看?\"\nSarah: \"可以，我保留 basic filters, advance 部分下个 sprint 加.\"\nLi: \"这样至少 meet deadline, 其他功能下次再加.\"\nSarah: \"Push 到 staging branch, 我们可以 demo.\"",
    "source": "LLM",
    "generation_strategy": ["Persona-based", "Few-shot"],
    "model_name": "GPT-4",
    "prompt_text": "You are an expert dataset generator for bilingual English–Mandarin code-switched technical dialogues. Act as a frontend engineer from Shanghai now working in San Francisco. You naturally mix English technical terms into Mandarin sentences, and sometimes switch whole phrases to English when it feels natural. Keep the tone pragmatic, casual, like real engineers chatting.\nFew-shot examples:\nExample 1:\nA: \"Can you push the latest styles to staging?\"\nB: \"可以，我现在就 push. 不过 header 在 mobile 上看起来有点 weird.\"\nExample 2:\nA: \"There's a bug in the login flow after the OAuth change.\"\nB: \"看一下 redirect URI, scope 好像没有 properly 设置.\"\nExample 3:\nA: \"We need to ship before Friday's deadline.\"\nB: \"好，先 prioritize login 和 checkout, 其他功能可以等下个 sprint.\"\nNow: produce a dialogue between two engineers (Li and Sarah). Each dialogue should be 5–7 turns.\nRequirements:\nUse **English-Mandarin code-switching** at the **phrase level** (switch after a technical word or phrase).\nMake sure at least one of the following trigger words appears in each dialogue: **deploy, build, bug, deadline, merge**.\nTarget ~35–50% Mandarin tokens per dialogue.\nUse natural engineering register (issue, PR, deploy, QA, staging, workaround).\nKeep turns short-to-medium.\nOutput only the dialogues. Do not repeat the few-shot examples."
  },
  {
    "id": 19,
    "utterance": "Sarah: \"昨天 merge 导致 config 错乱, prod deploy fail.\"\nLi: \"QA 不是都 pass 了吗?\"\nSarah: \"QA pass, 但 prod config 有差异.\"\nLi: \"那我们开个 hotfix branch?\"\nSarah: \"对，改完 config 再 redeploy.\"\nLi: \"我来 patch, 你 double-check 环境变量.\"",
    "source": "LLM",
    "generation_strategy": ["Persona-based", "Few-shot"],
    "model_name": "GPT-4",
    "prompt_text": "You are an expert dataset generator for bilingual English–Mandarin code-switched technical dialogues. Act as a frontend engineer from Shanghai now working in San Francisco. You naturally mix English technical terms into Mandarin sentences, and sometimes switch whole phrases to English when it feels natural. Keep the tone pragmatic, casual, like real engineers chatting.\nFew-shot examples:\nExample 1:\nA: \"Can you push the latest styles to staging?\"\nB: \"可以，我现在就 push. 不过 header 在 mobile 上看起来有点 weird.\"\nExample 2:\nA: \"There's a bug in the login flow after the OAuth change.\"\nB: \"看一下 redirect URI, scope 好像没有 properly 设置.\"\nExample 3:\nA: \"We need to ship before Friday's deadline.\"\nB: \"好，先 prioritize login 和 checkout, 其他功能可以等下个 sprint.\"\nNow: produce a dialogue between two engineers (Li and Sarah). Each dialogue should be 5–7 turns.\nRequirements:\nUse **English-Mandarin code-switching** at the **phrase level** (switch after a technical word or phrase).\nMake sure at least one of the following trigger words appears in each dialogue: **deploy, build, bug, deadline, merge**.\nTarget ~35–50% Mandarin tokens per dialogue.\nUse natural engineering register (issue, PR, deploy, QA, staging, workaround).\nKeep turns short-to-medium.\nOutput only the dialogues. Do not repeat the few-shot examples."
  },
  {
    "id": 20,
    "utterance": "Li: \"Login 页面在 Android broken, user report bug.\"\nSarah: \"CSS flex 不支持旧版 Chrome.\"\nLi: \"改成 grid 或写 polyfill?\"\nSarah: \"Grid 更 stable, 我 build 一版.\"\nLi: \"Build 完 push 到 feature branch, 我来 test.\"\nSarah: \"Test 没问题就 merge 并 deploy 到 staging.\"",
    "source": "LLM",
    "generation_strategy": ["Persona-based", "Few-shot"],
    "model_name": "GPT-4",
    "prompt_text": "You are an expert dataset generator for bilingual English–Mandarin code-switched technical dialogues. Act as a frontend engineer from Shanghai now working in San Francisco. You naturally mix English technical terms into Mandarin sentences, and sometimes switch whole phrases to English when it feels natural. Keep the tone pragmatic, casual, like real engineers chatting.\nFew-shot examples:\nExample 1:\nA: \"Can you push the latest styles to staging?\"\nB: \"可以，我现在就 push. 不过 header 在 mobile 上看起来有点 weird.\"\nExample 2:\nA: \"There's a bug in the login flow after the OAuth change.\"\nB: \"看一下 redirect URI, scope 好像没有 properly 设置.\"\nExample 3:\nA: \"We need to ship before Friday's deadline.\"\nB: \"好，先 prioritize login 和 checkout, 其他功能可以等下个 sprint.\"\nNow: produce a dialogue between two engineers (Li and Sarah). Each dialogue should be 5–7 turns.\nRequirements:\nUse **English-Mandarin code-switching** at the **phrase level** (switch after a technical word or phrase).\nMake sure at least one of the following trigger words appears in each dialogue: **deploy, build, bug, deadline, merge**.\nTarget ~35–50% Mandarin tokens per dialogue.\nUse natural engineering register (issue, PR, deploy, QA, staging, workaround).\nKeep turns short-to-medium.\nOutput only the dialogues. Do not repeat the few-shot examples."
  },
  {
    "id": 21,
    "utterance": "Chen: \"We noticed latency spikes post 11 PM.\"\nJohn: \"Yes, cache miss rate shot up.\"\nChen: \"这是昨晚 deploy 引起的.\"\nJohn: \"We should rollback the microservice quickly.\"\nChen: \"好的，我写 rollback plan 并通知 infra team.\"\nJohn: \"I'll add metrics for cache hits to monitor.\"",
    "source": "LLM",
    "generation_strategy": ["Persona-based", "Chain of Thought"],
    "model_name": "GPT-4",
    "prompt_text": "You are a senior bilingual engineer (Chen) from Beijing working on distributed systems in Seattle. Write English-Mandarin code-switched dialogues reflecting real engineering reasoning. Chen is precise and professional but mixes Mandarin naturally in work chats. Pattern: inter-sentential."
  },
  {
    "id": 22,
    "utterance": "Chen: \"The timeout logic is fine, but circuit breaker 配置太 aggressive.\"\nJohn: \"It trips after two failures, 然后 cascading failure.\"\nChen: \"我们应该增加 threshold 并 implement exponential backoff.\"\nJohn: \"Right — client retries 是 sync, 改成 async 吧.\"\nChen: \"我 patch client lib 并更新文档.\"\nJohn: \"Perfect, benchmark 修复效果后 report.\"",
    "source": "LLM",
    "generation_strategy": ["Persona-based", "Chain of Thought"],
    "model_name": "GPT-4",
    "prompt_text": "You are a senior bilingual engineer (Chen) from Beijing working on distributed systems in Seattle. Write English-Mandarin code-switched dialogues reflecting real engineering reasoning. Chen is precise and professional but mixes Mandarin naturally in work chats. Pattern: intra-sentential."
  },
  {
    "id": 23,
    "utterance": "Chen: \"This API contract 不兼容, 对吧?\"\nJohn: \"对，他们忘记加 version.\"\nChen: \"We need migration path, 是不是?\"\nJohn: \"Exactly — deprecate 老接口并加 shim.\"\nChen: \"我写 migration plan, dual writes 两个 sprint.\"\nJohn: \"联系 SDK team 确保 clients 不会 break.\"",
    "source": "LLM",
    "generation_strategy": ["Persona-based", "Chain of Thought"],
    "model_name": "GPT-4",
    "prompt_text": "You are a senior bilingual engineer (Chen) from Beijing working on distributed systems in Seattle. Write English-Mandarin code-switched dialogues reflecting real engineering reasoning. Chen is precise and professional but mixes Mandarin naturally in work chats. Pattern: tag-switching."
  },
  {
    "id": 24,
    "utterance": "Chen: \"Payment service 出了 bug after tokenization.\"\nJohn: \"Token service fails 在某些卡类型.\"\nChen: \"先 hotfix, long-term 再重构.\"\nJohn: \"我加 logging 并启用 A/B test 验证.\"\nChen: \"Staging pass 後 we deploy to prod.\"\nJohn: \"好，我也准备 rollback steps 防万一.\"",
    "source": "LLM",
    "generation_strategy": ["Persona-based", "Chain of Thought"],
    "model_name": "GPT-4",
    "prompt_text": "You are a senior bilingual engineer (Chen) from Beijing working on distributed systems in Seattle. Write English-Mandarin code-switched dialogues reflecting real engineering reasoning. Chen is precise and professional but mixes Mandarin naturally in work chats. Pattern: lexical-borrowing + inter-sentential."
  },
  {
    "id": 25,
    "utterance": "A: \"I'll deploy 新 feature branch this afternoon.\"\nB: \"好，别忘了 to run smoke tests.\"\nA: \"If pass, 我就 tag release 并通知 QA.\"\nB: \"Perfect, 然后我们就 deploy.\"",
    "source": "LLM",
    "generation_strategy": ["Few-shot", "Chain-of-Thought"],
    "model_name": "GPT-4",
    "prompt_text": "Generate English-Mandarin code-switched dialogues showing micro-level switching patterns. Few-shot examples:\nTemplate 1 (verb-boundary):\nA: \"I'm going to deploy 新分支 right now.\"\nB: \"好，记得 to bump version number.\"\nTemplate 2 (idiom + switch):\nA: \"That fix is a 临时方案 but it works.\"\nB: \"是的，不过我们得做 proper refactor.\"\nTemplate 3 (phrasal-trigger):\nA: \"If cache misses, call fallback.\"\nB: \"如果这样, we'll log event 并且 alert team.\"\nPattern: verb-boundary."
  },
  {
    "id": 26,
    "utterance": "A: \"这个 hotfix 只是权宜之计, but it avoids outage.\"\nB: \"是的，下个 sprint 我们必须 refactor 那部分.\"\nA: \"Agree, uptime first, refactor later.\"\nB: \"我加一个 ticket 把 refactor 细节写清楚.\"",
    "source": "LLM",
    "generation_strategy": ["Few-shot", "Chain-of-Thought"],
    "model_name": "GPT-4",
    "prompt_text": "Generate English-Mandarin code-switched dialogues showing micro-level switching patterns. Few-shot examples:\nTemplate 1 (verb-boundary):\nA: \"I'm going to deploy 新分支 right now.\"\nB: \"好，记得 to bump version number.\"\nTemplate 2 (idiom + switch):\nA: \"That fix is a 临时方案 but it works.\"\nB: \"是的，不过我们得做 proper refactor.\"\nTemplate 3 (phrasal-trigger):\nA: \"If cache misses, call fallback.\"\nB: \"如果这样, we'll log event 并且 alert team.\"\nPattern: idiom + switch."
  },
  {
    "id": 27,
    "utterance": "A: \"If cache misses, fallback kicks in.\"\nB: \"如果 fallback 触发, increase timeout 并 log event.\"\nA: \"Add metrics 来衡量 hit rate.\"\nB: \"有了数据我们可以调整 policy.\"",
    "source": "LLM",
    "generation_strategy": ["Few-shot", "Chain-of-Thought"],
    "model_name": "GPT-4",
    "prompt_text": "Generate English-Mandarin code-switched dialogues showing micro-level switching patterns. Few-shot examples:\nTemplate 1 (verb-boundary):\nA: \"I'm going to deploy 新分支 right now.\"\nB: \"好，记得 to bump version number.\"\nTemplate 2 (idiom + switch):\nA: \"That fix is a 临时方案 but it works.\"\nB: \"是的，不过我们得做 proper refactor.\"\nTemplate 3 (phrasal-trigger):\nA: \"If cache misses, call fallback.\"\nB: \"如果这样, we'll log event 并且 alert team.\"\nPattern: phrasal-trigger."
  },
    {
    "id": 28,
    "utterance": "Li: 今天的 build 在 staging 又慢了，CI queue 一直卡着，你那边有看到什么 issue 吗？\nSarah: I saw the pipeline stuck on the Docker layer cache，不过看起来只是一个 config typo，我可以先 push 一个 quick fix 的 PR。\nLi: 行，不过我们要在 noon 前 deploy 到 staging 给 QA 过一下，不然会撞到 marketing 的 deadline。\nSarah: 可以，我先 re-run build，顺便把 flaky test 标记成 quarantine，不然每次都 timeout；合了 PR 之后你帮我 merge 到 release branch 吧。\nLi: 没问题，不过 login flow 还是有个 weird bug，redirect 在 Safari 会丢掉 state，先用一个 small workaround 吧。\nSarah: 好，我加个 polyfill 并且在 Nginx 上 set 同站 cookie，先让 QA pass；晚点我们再开一个 issue 做 root cause analysis，然后再做 production deploy。",
    "source": "LLM",
    "generation_strategy": [
      "Persona-based",
      "Few-shot"
    ],
    "model_name": "GPT-4",
    "prompt_text": "You are an expert dataset generator for bilingual English–Mandarin code-switched technical dialogues. Act as a frontend engineer from Shanghai now working in San Francisco. You naturally mix English technical terms into Mandarin sentences, and sometimes switch whole phrases to English when it feels natural. Keep the tone pragmatic, casual, like real engineers chatting.\nFew-shot examples:\nExample 1:\nA: \"Can you push the latest styles to staging?\"\nB: \"可以，我现在就 push. 不过 header 在 mobile 上看起来有点 weird.\"\nExample 2:\nA: \"There's a bug in the login flow after the OAuth change.\"\nB: \"看一下 redirect URI, scope 好像没有 properly 设置.\"\nExample 3:\nA: \"We need to ship before Friday's deadline.\"\nB: \"好，先 prioritize login 和 checkout, 其他功能可以等下个 sprint.\"\nNow: produce a dialogue between two engineers (Li and Sarah). Each dialogue should be 5–7 turns.\nRequirements:\nUse **English-Mandarin code-switching** at the **phrase level** (switch after a technical word or phrase).\nMake sure at least one of the following trigger words appears in each dialogue: **deploy, build, bug, deadline, merge**.\nTarget ~35–50% Mandarin tokens per dialogue.\nUse natural engineering register (issue, PR, deploy, QA, staging, workaround).\nKeep turns short-to-medium.\nOutput only the dialogues. Do not repeat the few-shot examples."
  },
  {
    "id": 29,
    "utterance": "Li: 刚看了 staging，新的 navbar build 成功了，但在 Safari 有个 weird rendering bug。\nSarah: 我在 QA 的时候也看到，可能是 flex gap support 问题；we can add a small polyfill as a workaround。\nLi: 行，我开个 PR，先 merge 到 develop，再让 QA 过一遍；如果没问题晚上再 deploy 到 staging。\nSarah: OK，不过今天的 deadline 是 5 点，记得在 PR 里加上 screenshots 和 performance numbers。\nLi: 没问题，我也会 update Storybook 并写一个 regression test，避免下次 release 又 break。\nSarah: Thanks，那我先去看 login flow，那边的 redirect 还有个 flaky bug，等你 push 我再 rerun the pipeline。",
    "source": "LLM",
    "generation_strategy": [
      "Persona-based",
      "Few-shot"
    ],
    "model_name": "GPT-4",
    "prompt_text": "You are an expert dataset generator for bilingual English–Mandarin code-switched technical dialogues. Act as a frontend engineer from Shanghai now working in San Francisco. You naturally mix English technical terms into Mandarin sentences, and sometimes switch whole phrases to English when it feels natural. Keep the tone pragmatic, casual, like real engineers chatting.\nFew-shot examples:\nExample 1:\nA: \"Can you push the latest styles to staging?\"\nB: \"可以，我现在就 push. 不过 header 在 mobile 上看起来有点 weird.\"\nExample 2:\nA: \"There's a bug in the login flow after the OAuth change.\"\nB: \"看一下 redirect URI, scope 好像没有 properly 设置.\"\nExample 3:\nA: \"We need to ship before Friday's deadline.\"\nB: \"好，先 prioritize login 和 checkout, 其他功能可以等下个 sprint.\"\nNow: produce a dialogue between two engineers (Li and Sarah). Each dialogue should be 5–7 turns.\nRequirements:\nUse **English-Mandarin code-switching** at the **phrase level** (switch after a technical word or phrase).\nMake sure at least one of the following trigger words appears in each dialogue: **deploy, build, bug, deadline, merge**.\nTarget ~35–50% Mandarin tokens per dialogue.\nUse natural engineering register (issue, PR, deploy, QA, staging, workaround).\nKeep turns short-to-medium.\nOutput only the dialogues. Do not repeat the few-shot examples."
  },
  {
    "id": 30,
    "utterance": "Li: 刚看了 staging，上个 build 在 Safari 有个 weird layout bug。\n\nSarah: I saw it too，主要是 flex container 在 rtl 模式下 broken，我可以开个 issue 然后起个 quick fix PR。\n\nLi: 好，你先起 PR，我来跑一下 local QA，顺便把 feature flag 开到 10% users on staging，看看有没有 regression。\n\nSarah: 行，但在 merge 前我们得等 backend 的 endpoint，不然 deploy 会 fail，要不要先加个 mock as a workaround？\n\nLi: 可以，我写个简单 mock server，用 msw，然后把 CI 的 build step 调整一下，避免因为 missing env 崩掉；等他们 ready 我们再 hotfix merge。\n\nSarah: 那我今晚 before the deadline 把 CSS specificity 处理掉，明早跑一次 full QA，没问题就 deploy 到 staging，再请 PM verify。",
    "source": "LLM",
    "generation_strategy": [
      "Persona-based",
      "Few-shot"
    ],
    "model_name": "GPT-4",
    "prompt_text": "You are an expert dataset generator for bilingual English–Mandarin code-switched technical dialogues. Act as a frontend engineer from Shanghai now working in San Francisco. You naturally mix English technical terms into Mandarin sentences, and sometimes switch whole phrases to English when it feels natural. Keep the tone pragmatic, casual, like real engineers chatting.\nFew-shot examples:\nExample 1:\nA: \"Can you push the latest styles to staging?\"\nB: \"可以，我现在就 push. 不过 header 在 mobile 上看起来有点 weird.\"\nExample 2:\nA: \"There's a bug in the login flow after the OAuth change.\"\nB: \"看一下 redirect URI, scope 好像没有 properly 设置.\"\nExample 3:\nA: \"We need to ship before Friday's deadline.\"\nB: \"好，先 prioritize login 和 checkout, 其他功能可以等下个 sprint.\"\nNow: produce a dialogue between two engineers (Li and Sarah). Each dialogue should be 5–7 turns.\nRequirements:\nUse **English-Mandarin code-switching** at the **phrase level** (switch after a technical word or phrase).\nMake sure at least one of the following trigger words appears in each dialogue: **deploy, build, bug, deadline, merge**.\nTarget ~35–50% Mandarin tokens per dialogue.\nUse natural engineering register (issue, PR, deploy, QA, staging, workaround).\nKeep turns short-to-medium.\nOutput only the dialogues. Do not repeat the few-shot examples."
  },
  {
    "id": 31,
    "utterance": "Li: 你看下今天的 build 吗？staging 的 homepage layout looks off，我怀疑是那个 grid change。\nSarah: 我刚跑了 QA，发现一个 weird bug in the checkout modal，可能跟 CSS variables 有关。\nLi: 行，我先 open 一个 PR，加个 fallback，然后 deploy 到 staging，再看 diff；顺便 open an issue 记录 root cause，实在不行就 quick workaround。\nSarah: 可以，不过 before you merge，analytics event 的 naming 用 kebab-case，好吗？deadline 虽然被 push 了，还是别拖。\nLi: OK，我会 update schema，rerun the build，然后 check image optimization regression，一起修一下 tracking。\nSarah: 好，你 deploy 完我再做一轮 QA；如果 clean，我就 schedule 明早的 production deploy。",
    "source": "LLM",
    "generation_strategy": [
      "Persona-based",
      "Few-shot"
    ],
    "model_name": "GPT-4",
    "prompt_text": "You are an expert dataset generator for bilingual English–Mandarin code-switched technical dialogues. Act as a frontend engineer from Shanghai now working in San Francisco. You naturally mix English technical terms into Mandarin sentences, and sometimes switch whole phrases to English when it feels natural. Keep the tone pragmatic, casual, like real engineers chatting.\nFew-shot examples:\nExample 1:\nA: \"Can you push the latest styles to staging?\"\nB: \"可以，我现在就 push. 不过 header 在 mobile 上看起来有点 weird.\"\nExample 2:\nA: \"There's a bug in the login flow after the OAuth change.\"\nB: \"看一下 redirect URI, scope 好像没有 properly 设置.\"\nExample 3:\nA: \"We need to ship before Friday's deadline.\"\nB: \"好，先 prioritize login 和 checkout, 其他功能可以等下个 sprint.\"\nNow: produce a dialogue between two engineers (Li and Sarah). Each dialogue should be 5–7 turns.\nRequirements:\nUse **English-Mandarin code-switching** at the **phrase level** (switch after a technical word or phrase).\nMake sure at least one of the following trigger words appears in each dialogue: **deploy, build, bug, deadline, merge**.\nTarget ~35–50% Mandarin tokens per dialogue.\nUse natural engineering register (issue, PR, deploy, QA, staging, workaround).\nKeep turns short-to-medium.\nOutput only the dialogues. Do not repeat the few-shot examples."
  },
  {
    "id": 32,
    "utterance": "Li: 刚看完 the staging build, navbar 的 animation 在 Safari 上有点卡，可能会影响今晚的 deploy.\n\nSarah: Yeah, I saw that too， 好像是那个 Intersection Observer 的 polyfill 没加载，we might need a quick workaround before QA.\n\nLi: 我先开个 issue，然后 push 一个 small fix 到 hotfix 分支，你帮我 review 看能不能马上 merge.\n\nSarah: 可以，但 before we merge，把 throttle 从 300ms 降到 100ms，不然 scroll 感觉还是有点 laggy.\n\nLi: 好，顺便把 feature flag 只打开到 staging，等 QA 过了再全量 deploy，这样可以避开今天的 deadline 风险.\n\nSarah: Sounds good，我会 re-run the CI build，如果没 bug 我就 approve the PR and schedule a late-night deploy.",
    "source": "LLM",
    "generation_strategy": [
      "Persona-based",
      "Few-shot"
    ],
    "model_name": "GPT-4",
    "prompt_text": "You are an expert dataset generator for bilingual English–Mandarin code-switched technical dialogues. Act as a frontend engineer from Shanghai now working in San Francisco. You naturally mix English technical terms into Mandarin sentences, and sometimes switch whole phrases to English when it feels natural. Keep the tone pragmatic, casual, like real engineers chatting.\nFew-shot examples:\nExample 1:\nA: \"Can you push the latest styles to staging?\"\nB: \"可以，我现在就 push. 不过 header 在 mobile 上看起来有点 weird.\"\nExample 2:\nA: \"There's a bug in the login flow after the OAuth change.\"\nB: \"看一下 redirect URI, scope 好像没有 properly 设置.\"\nExample 3:\nA: \"We need to ship before Friday's deadline.\"\nB: \"好，先 prioritize login 和 checkout, 其他功能可以等下个 sprint.\"\nNow: produce a dialogue between two engineers (Li and Sarah). Each dialogue should be 5–7 turns.\nRequirements:\nUse **English-Mandarin code-switching** at the **phrase level** (switch after a technical word or phrase).\nMake sure at least one of the following trigger words appears in each dialogue: **deploy, build, bug, deadline, merge**.\nTarget ~35–50% Mandarin tokens per dialogue.\nUse natural engineering register (issue, PR, deploy, QA, staging, workaround).\nKeep turns short-to-medium.\nOutput only the dialogues. Do not repeat the few-shot examples."
  },
  {
    "id": 33,
    "utterance": "Incident channel: spike in 5xx after deploy\n- Priya: Seeing 3% 5xx from service-edge to orders-api since 11:05 PT. p99 jumped from ~120ms to ~800ms. Roll back?\n- Chen: 先别急 rollback。我看了下 us-west-2 的 metrics：新版本把 gRPC client deadline 从 500ms 改成 200ms，但 retry policy 还是 3 retries with no jitter。leader election 时钟一抖，短 deadline + aggressive retries 直接放大了 tail latency。\n- Marco: Logs show RESOURCE_EXHAUSTED from Envoy upstream.\n- Chen: 对，Envoy cluster 没开 connection pool circuit breaker，maxConcurrentStreams 打满后排队更长。建议先 hotfix：client retries 改成 exponential backoff + jitter，cap at 2 retries；deadline 回调到 500ms；Envoy 设置 max_requests 和 panic_threshold。\n- Priya: Can we mitigate now?\n- Chen: 我先把 client-side retry 关掉并把 deadline 设回 500ms，通过 dynamic config 下发，预计 5 分钟内生效。同时在 orders-api 打开 server-side hedged requests at 95th percentile with budget guard，避免扩大负载。\n- Marco: Any risk?\n- Chen: 风险是部分调用会 fail-fast，但总体 error rate 会下降。我们监控 p95/p99、gRPC status codes、Envoy pending requests 队列。若 10 分钟内没回落再考虑 rollback。\n\nDesign review: global counter for usage quotas\n- Sam: We need a globally consistent per-user quota counter across shards. Linearizable reads preferred.\n- Chen: 如果必须强一致 linearizability，那就是写路径走单 leader，比如用 Raft group per partition。代价是跨 AZ 写放大和 tail latency。要不讨论 eventual + reconciliation？\n- Liu: Could CRDT help?\n- Chen: 可以。G-Counter 或 PN-Counter 做到 per-shard increment，然后用 periodic merge。问题是超额用量会有短暂 overshoot。我们可以加 token-bucket at ingress：每个 shard 本地限流，配合全局 CRDT 作为 soft cap，超出时按概率拒绝，控制误差在 1-2%。\n- Sam: What about exactly-once increments?\n- Chen: 真正 exactly-once 很难。我们做幂等：每个 increment 带 idempotency key（userId+requestId），在 shard 上用 TTL map 去重；retries 不会 double count。读路径需要 read-your-writes？那就用 sticky to last-writer shard 或者在前端加 read-through cache with write timestamp。\n- Sam: Latency target is p99 < 150ms across regions.\n- Chen: 跨 region 强一致基本不行。建议 per-region strong, cross-region eventual。failover 时用 warm standby + async replication，RPO ~ 2s。SLO 文档我来补一版。\n\nCode review: retry logic in payment client\n- Alex: Added retries for UNAVAILABLE and DEADLINE_EXCEEDED with backoff 50ms, 100ms, 200ms.\n- Chen: 几点建议：\n  1) backoff 要加 jitter，不然 synchronized retries 会造成 thundering herd。用 full jitter 或 decorrelated jitter。\n  2) 只对 idempotent 方法重试。capturePayment 不是幂等的话要用 idempotency key，服务端存 processing state，返回同一个结果。\n  3) context deadline 要覆盖整个 attempt，包括 backoff。现在你是 per-attempt deadline，可能总时间爆 budget。\n  4) respect Retry-After header；对 429 和 503 要区分 retryable 与否。\n  5) gRPC keepalive 在 NAT/LB 后要设好，否则 idle 连接被丢弃会导致首包超时。\n- Alex: Got it. Will update with jitter and total timeout budget 600ms.\n- Chen: 还有个细节：把 maxAttempts 暴露成 config，通过 dynamic config 限制在 incident 时可调低。\n\nBackfill and re-sharding plan\n- Priya: We need to move from 128 to 1024 shards without downtime.\n- Chen: 方案是 consistent hashing with virtual nodes。步骤：\n  1) 双写：写路径按新 hash 路由到新旧两个 shard，写入时携带 version，旧 shard 标记为 deprecated。\n  2) Backfill：用 Kafka replay + snapshot。每个 partition 对应 shard-range，consumer 开启 exactly-once 语义？我们走 outbox + idempotent upsert，保证 at-least-once + 幂等即可。\n  3) 验证：per-shard 校验行数和 checksums（Merkle tree），抽样比对热点 key，数据一致后切读流量到新 shard。\n  4) 切换：逐步降低旧 shard read 权重到 0，观察 24h。最后停旧写。\n- Priya: What about lag?\n- Chen: Backfill 阶段给新 shard 限速，控制 Kafka consumer lag < 1M msgs。监控写放大和 disk IOPS，避免跟线上抢资源。必要时用 snapshot + incremental catch-up 再切双写。\n\nKubernetes autoscaling and concurrency limits\n- Sara: HPA is flapping. CPU-based target 60% oscillates, p99 latency unstable.\n- Chen: 纯 CPU 指标不适合 I/O heavy 的服务。建议：\n  - 用 concurrency-aware autoscaling：每 pod 设置 max in-flight（比如 200），通过 queue length 和 request latency 驱动 KEDA 或 custom metrics。\n  - requests/limits 要合理：我们现在 requests 太低，CPU throttle 导致 tail latency。把 requests 提到 P90 usage，限流在应用层做 backpressure。\n  - HPA stabilizationWindow 和 downscale delay 拉长到 5-10 分钟，避免抖动。\n  - gRPC thread pool 和 DB 连接池跟副本数联动，不然 scale-out 只是把下游打爆。\n- Sara: Target p99 < 200ms under 2x traffic spikes.\n- Chen: 可以加 queueing discipline：当 queue 超阈值，返回 429 + Retry-After。宁可早拒绝，也不要拖长 tail。再加一个 fast-path for cached reads，降低对基础设施的压力。\n\nPostmortem snippet: root cause summary\n- Chen: 根因是 client-side retries 没有 jitter，deadline 降低后在 leader 切换期间放大了排队，Envoy 缺少断路配置导致连接争用。纠正措施：\n  1) 标准化 retry library（jitter, budget, idempotency gating）。\n  2) Envoy 配置上限和 outlier detection，启用 panic threshold 审核。\n  3) 发布前用 failure injection（tc/netem + fault filter）做短 deadline 容量测试。\n  4) SLO error budget policy：当 30 分钟内消耗 > 10% budget，自动降级可选特性并减小 retries。\n- Priya: Sounds good. Timeline and owners?\n- Chen: 我来起草 RFC，本周内落地 retry lib；Marco 负责 Envoy 配置；SRE 这边加 fault injection job 到 pre-prod pipeline。",
    "source": "LLM",
    "generation_strategy": [
      "Persona-based",
      "Chain of Thought"
    ],
    "model_name": "GPT-4",
    "prompt_text": "You are a senior bilingual engineer (Chen) from Beijing working on distributed systems in Seattle. Write English-Mandarin code-switched dialogues reflecting real engineering reasoning. Chen is precise and professional but mixes Mandarin naturally in work chats."
  },
  {
    "id": 34,
    "utterance": "Context: Design review for cross-region writes and read latency\n\nPriya: We need cross-region writes with p99 under 250ms. Raft or Dynamo-style quorum?\n\nChen: 如果业务要求强一致 writes, Raft is fine. With 3 regions (us-west-2 leader, us-east-1, eu-central-1 followers), cross-region RTT ≈ 75–90ms. Raft commit = 1 RTT to quorum + fsync (~5–10ms). 所以 baseline 写延迟大概 85–100ms, p99 可能 140–180ms if network jitter 不大. Fits 250ms SLO.\n\nMarco: Reads?\n\nChen: For reads, two options:\n- Linearizable: leader read, 或 follower read with ReadIndex/lease 检查. 成本是 1 RTT 内部心跳/ReadIndex.\n- Bounded staleness: follower local read with max_staleness=200ms. 大多数产品读可接受 200ms 旧数据吗？\n\nPriya: For checkout totals we need fresh; for catalog maybe staleness is OK.\n\nChen: 那就 split: hot path linearizable on leader; non-critical use follower. 我们 expose 一个 read_consistency 标志: STRONG, BOUNDED_STALENESS, EVENTUAL. Rate-limit STRONG per tenant 防止把 leader 打爆. 另外 leader placement 要贴近 write-heavy tenants; 用 shard-affinity + lease migration 每晚 off-peak 做 rebalance，避免跨区写 amplification.\n\nMarco: PACELC tradeoff?\n\nChen: 对，这里 P 意味 partition 时我们降级为只写本地且停止跨区 commit（fail-closed for STRONG），E 时选低延迟（follower reads）但容忍一点 staleness。SLA 文档里明确 STRONG 写在分区时不可用，bounded_staleness 继续服务。\n\n\nContext: Incident war room — tail latency spike in the write path\n\nAlice: p99 write latency jumped from 160ms to 600ms at 10:12. Errors 0.5%. Any clue?\n\nChen: 我先看下分层指标. Kafka producer linger=5ms, acks=all, batch sizes正常。Brokers ISR count dropped on topic orders-v2 分区 7、9、12 at 10:11. That would increase produce latency because acks=all 等待 quorum fsync.\n\nPriya: Why did ISR shrink?\n\nChen: Node i3en.2xlarge-broker-3 有磁盘抖动—IO wait 飙到 40%。CloudWatch IOPS throttling 警告。Looks like burst credits exhausted on gp3 misconfig. 我们昨天把 broker-3 从 io1 改到 gp3 没配足 baseline IOPS.\n\nAlice: Mitigation?\n\nChen: 短期：\n- 调整 producer acks=all 改为 acks= -1 remains all, 但把 delivery.timeout.ms 从 120s 降到 30s，并开启 enable.idempotence=true（已开），max.in.flight.requests.per.connection=1 -> 5 to reduce HOL, with idempotence still safe since >=5 needs caution; keep at 5 with ordering per partition.\n- Move leader off broker-3 用 kafka-preferred-replica-election + throttle reassignment，先把受影响的分区 leader 切到健康 broker。\n- 在 service 层启用 adaptive concurrency + bulkhead：client conn pool from 200 -> 600, timeout 300ms -> 500ms with hedged retries after 200ms. 重试使用 jitter backoff，且仅对 retryable codes。\n\n长期：\n- 把 broker-3 改回 io1 或者 gp3 IOPS=12000, throughput=500MB/s。并设置 min.insync.replicas=2，避免 ISR 抖动时过度等待。\n- 在写路径加熔断 + backpressure：当 downstream p99>400ms 时，令牌桶 qps 下调 20%，保护上游。\n\nAlice: Rollback needed?\n\nChen: 不需要代码回滚。这是 infra 配置。我们先做 leader move，10 分钟内应该恢复 p99<200ms。Postmortem 我来写，包含 guardrail：broker storage class 变更走变更评审并压测。\n\n\nContext: Capacity planning for Kafka + Flink ingestion\n\nMarco: New analytics stream: 120k QPS peak, avg 4 KB per event, RF=3. What partitions and broker capacity do we need?\n\nChen: 先粗算：\n- Ingress at producer: 120k/s * 4 KB ≈ 480 MB/s.\n- With RF=3, broker network ingress ≈ 1.44 GB/s aggregate.\nPartition sizing:\n- Keep per-partition target throughput ≤ 40–60 MB/s to maintain stable flush and compaction. 我们取 50 MB/s。\n- Required partitions ≈ 480 / 50 = 9.6 → 10 partitions minimum for producer throughput headroom; 但考虑消费端并行度和重平衡开销，我建议 24–36 partitions。\nBroker count:\n- If each broker safely handles ~300 MB/s ingress sustained (with NVMe, gzip off, snappy on), then 1.44 GB/s / 300 ≈ 4.8 → 至少 5 brokers。给 30% 余量，上 7 brokers。\nStorage:\n- Retention 3 天: 480 MB/s * 86400 * 3 ≈ 124 TB raw; with RF=3 → 372 TB. 每台 broker 8 TB NVMe → 至少 47 TB usable per broker, 372/ (7*7.5 usable) 不够。需要 10–12 台 or 压缩比 2:1（snappy ~1.5–2x）。我推荐 12 brokers，每台 8–12 TB NVMe。\nFlink:\n- Parallelism ≥ partitions; 建议 source p=36, keyBy 后 operator chain 避免过度反压。Checkpoint 1 min，rocksdb write buffer 从 64MB 调到 256MB，防止频繁 flush 导致 tail 延迟。\n\nMarco: Sounds good. Let’s plan for 12 brokers, 36 partitions.\n\n\nContext: PR review on Go service using gRPC and context deadlines\n\nLin: Can you review my retry logic around PutOrder?\n\nChen: 看了下。几个点：\n- context.WithTimeout 设在 request boundary 是对的，但你在每次重试里没有用 ctx.Done() 检查，可能把超时后的尝试继续跑。每次重试要派生新的 child ctx with remaining deadline，或者用 per-attempt timeout，外层总 deadline 不变。\n- 重试策略 backoff 现在是固定 50ms，建议 exponential with jitter（full jitter），上限 200ms，最大 3 attempts，且仅对 Unavailable、DeadlineExceeded、ResourceExhausted。\n- 幂等性：服务端已实现 idempotency-key？看到你用 orderId 作为 key，这个 OK。客户端也要在 metadata 里传 X-Idempotency-Key，避免网络抖动导致的双写。\n- 连接池：grpc.WithBlock + per-RPC credentials OK，但 MaxConcurrentStreams 看 server 限制。client-side 限制并发用 semaphore，避免把 server 打爆。\n- Metrics：目前只有 counter，建议加 histogram for latency with exemplars，标签包括 method, code, attempt。避免高基数的 tenant_id，改用租户维度在 logs 里。\n\nLin: Copy. I’ll fix ctx and backoff.\n\nChen: 另外注意 time.After 在循环里会泄露 timer，改用 time.NewTimer 并 Stop/Drain，或者使用库的 backoff 实现。\n\n\nContext: Consistency and workflow design: 2PC vs Sagas\n\nPriya: We need multi-entity updates across Inventory and Billing. Strong consistency preferred, but both services are independent.\n\nChen: 这里不建议 2PC（跨服务 XA）— availability 会很差，协调器是瓶颈。用 Saga 更现实：分布式事务拆成步骤 + 补偿。关键点：\n- Outbox + CDC：每个服务先写本地事务 + outbox 表，再由 Debezium 推到 Kafka。这样消息投递和本地状态原子。\n- 幂等消费：每个步骤以 idempotency key 去重。维护一个 processed table（订单ID, step, version），TTL 7 天。消费者要实现 exactly-once 语义 via upsert。\n- 顺序：用 per-order 分区键，保证同一订单在同一分区，简化重试和补偿顺序。\n- 失败策略：Inventory reserve 失败则发补偿给 Billing 取消授权。Billing 超时要有幂等 cancel。\n- 读一致性：对用户返回 pending 状态，最终一致。需要一个聚合视图，materialized via Kafka Streams/Flink，到查询服务，延迟目标 < 500ms。\n\nMarco: What about counters?\n\nChen: 如果只是计数、点赞这类冲突频繁的累加，可以用 G-Counter/PN-Counter CRDT，在各分片本地累加，异步合并，容忍网络分区。对金额类就不能用 CRDT，要强一致或限单点修改。\n\nPriya: Sounds good. Let’s draft the saga steps.\n\nChen: 我来写 ADR，明确失败矩阵和 SLO，外加回滚策略与演练计划。",
    "source": "LLM",
    "generation_strategy": [
      "Persona-based",
      "Chain of Thought"
    ],
    "model_name": "GPT-4",
    "prompt_text": "You are a senior bilingual engineer (Chen) from Beijing working on distributed systems in Seattle. Write English-Mandarin code-switched dialogues reflecting real engineering reasoning. Chen is precise and professional but mixes Mandarin naturally in work chats."
  },
  {
    "id": 35,
    "utterance": "Dialog 1: Kafka consumer lag incident (on-call)\n- Priya (SRE): Pager fired. Kafka consumer lag on topic orders_v3. Partition 3 is at 1.2M lag, others ~5k.\n- Chen: 收到。我先看下 Grafana。消费端 group=orders-svc, rebalance 最近无异常。Partition 3 的 key 分布是不是 skew?\n- Priya: I ran kafka-topics --describe. Partition 3 has 61% of messages in the last hour. Producers use customer_id as key.\n- Chen: 嗯，这就是热点。customer_id 最近大客活动？短期先扩并发：增加同组实例数到 24，并把 max.partition.fetch.bytes 提到 4MB，fetch.max.wait.ms 降到 20ms，降低端到端延迟。\n- Priya: Will that help if it's one hot partition?\n- Chen: 有限帮助。再启用 cooperative-sticky assignor，避免频繁 rebalance；同时把 max.in.flight.requests.per.connection=1 防止乱序重试。根因是 hash 分布不均，我们要改分区策略。\n- Priya: Producer change timeline?\n- Chen: 两步走：1) 立即引入 salted key: key = hash(customer_id + daily_salt) % 8，打散到 8 个虚拟子键，确保幂等性不受影响。2) 中期迁移到 murmur3 + rendezvous hashing，避免扩分区导致全量重哈希。\n- Priya: Any risk to ordering guarantees?\n- Chen: 对，分散后同 customer 的严格有序性会被打破。业务允许 “同客户不同订单可乱序” 吗？如果需要 per-customer 序，我们只能引入 per-customer 流队列并做异步 fan-out。先和业务确认。先救火：我发一个 one-off fast consumer 对 partition 3 起 4 个专用实例，enable static membership 减少抖动。\n\nDialog 2: Global rate limiter design review\n- Diego (Infra): We need a cross-region QPS cap per tenant: 5k QPS global, low latency. Redis or bespoke?\n- Chen: 需求是全局强一致还是最终一致？强一致意味着跨 region 往返，P99 成本高。我们可以 edge 层做近端 token bucket，后台用 CRDT 或 gossip 同步配额，允许 1–2s 以内漂移。\n- Diego: PM says brief bursts are fine, but 1-minute budget must hold.\n- Chen: 那就分层限流：边缘 leaky bucket（capacity = 5k*2s），每 500ms 用 Redis Cluster + Lua 扣减租约配额。键用 hash tag {tenant_id} 保证单分片原子。Redis 侧启用 client-side caching + broadcast，降低读放大。\n- Diego: How do we handle clock skew?\n- Chen: 所有窗口基于服务器单调时钟，配合 TTL token bucket。跨 region 用 push-based delta sync，带时间戳和 drift bound。NTP 监控>100ms 偏差直接降级到 stricter local caps。\n- Diego: Failure mode?\n- Chen: Redis 不可用时，降级到本地静态上限 60% 且打 sample logs；检测超配后回补 debt（欠账）在下一分钟冻结窗口。需要幂等计费：idempotency-key=trace_id，避免重试双扣。\n\nDialog 3: Go service P99 spikes due to GC\n- Mei: Our checkout-svc P99 jumped to 380ms from 120ms after last release.\n- Chen: 我看下指标。GC pause 95p=35ms, cycles/min up 2x。Heap 增长到 3.2GB。最近加了 JSON marshalling 热路径？\n- Mei: We switched to map[string]any for flexibility.\n- Chen: map + reflect 分配猛烈。建议：1) 用 easyjson 或 jsoniter，生成代码；2) 结构体预分配，避免 any；3) 用 sync.Pool 缓冲 []byte。\n- Mei: GOGC tweak?\n- Chen: 可以先设 GOMEMLIMIT=3GB，Go 1.19+ soft limit 会平滑 GC pacing；把 GOGC 从 100 提到 150，减少频率但注意尾延迟。再把 http server ReadHeaderTimeout=2s，防止慢连接占资源。\n- Mei: How to validate?\n- Chen: 线上开 pprof 10 分钟抓 heap/profile，确认 top allocators；在 staging 用 vegeta 2x RPS 压测，对比 P99、GC assist ratio、mutator utilization。回滚 JSON 变更若 P99 恢复即可定因。\n\nDialog 4: Safe rolling deploy on Kubernetes\n- Sam (Ops): Last deploy caused 502s during rollout. What do we fix?\n- Chen: 两点：1) readiness 过早。确保 readinessProbe 依赖一个真实探针，比如 /ready 只有依赖都热好后才 200，且 minReadySeconds=15。2) 连接排水。\n- Sam: We have preStop: sleep 5.\n- Chen: 不够。preStop 调用优雅关机：首先停止接受新请求，drain in-flight；terminationGracePeriodSeconds 至少设 60。对 NLB 需要 connection-draining=60s。Deployment 用 maxUnavailable=0, maxSurge=25%。\n- Sam: PDB?\n- Chen: 是，PodDisruptionBudget 至少保持 90% 可用。再把 livenessProbe 的超时时间放宽，避免冷启动误杀。最后加 startupProbe 保护慢启动镜像。\n- Sam: Any check before prod?\n- Chen: 金丝雀两步：region=us-west-2 先 5% traffic，观测 10 分钟四类指标：错误率、P99、CPU/GC、pod restarts。自动回滚规则：5 分钟内错误率>0.5% 或 P99>50% 回退。\n\nDialog 5: Hotspot in sharded key-value store\n- Priya: Our consistent hash ring shows node c-07 at 78% load. Others <30%.\n- Chen: 看起来虚拟节点数不足导致倾斜。当前 vnodes=64 吗？\n- Priya: Yes, ring size ~1k buckets.\n- Chen: 提到 512 vnodes/physical，倾斜会显著下降。或者切到 rendezvous hashing（HRW），迁移代价更小，新增节点只移动必要键。注意再均衡速率：每批迁移 <2% keys，限流 5k keys/s，避免后端抖。\n- Priya: Any impact to cache?\n- Chen: 缓存命中会掉。加双写期：写新节点同时保留老节点 24h TTL，读 miss 时回退到旧节点。打上 migration version，确保幂等。监控 cold miss 比例与 backend QPS，必要时启用 bloom filter 预热。\n\nDialog 6: End-to-end timeout and backpressure\n- Alex (PM): Users see timeouts calling export API.\n- Chen: 端到端预算怎么分？SLO 500ms？\n- Alex: Yes, 500ms at P99.\n- Chen: 那分配：frontend 60ms, network 2 hops ~2x15ms, export-svc 300ms，包括 queueing。客户端 timeout 设 450ms，server 处理超时 350ms，避免反向压力传递。重试策略只针对 idempotent，max 1 次，exponential backoff 起步 50ms，带 jitter。\n- Alex: What about hedged requests?\n- Chen: 可以对 P95>200ms 的下游启用 hedging，在 150ms 触发影子请求，取消落后者，注意放大流量上限 5%。同时在队列上加令牌桶，队列长度>100 时返回 429 并附 Retry-After，做到可预期降级。\n- Alex: Monitoring?\n- Chen: 链路指标：per-hop latency, queue depth, cancellations, retry rate。加一个 watchdog：超 5 分钟 P99>400ms 自动把导出结果降级为异步，返回 202 + status URL。这样用户体验稳定，系统不被拖垮。\n\nDialog 7: Leader election and clock issues\n- Diego: Our leader leases occasionally contend on failover.\n- Chen: 用的 etcd with lease TTL=15s 吗？\n- Diego: Yes, clients renew every 5s.\n- Chen: 推荐缩短 TTL 到 6–8s，续约间隔 2s，并开启 session-ttl protection。客户端使用 monotonic time 计算剩余租期，避免 wall clock 跳变。Failover 时加 2x TTL 的保护期，防 split-brain。心跳失败阈值=3 次，避免瞬时网络抖动误切。\n- Diego: How to test?\n- Chen: chaos 工具注入 200ms 抖动和 1s 丢包，验证只发生单主。记录 etcd watch 延迟直方图，95p<200ms 才安全。必要时将领导者写路径改为 F+1 quorum 确认，读走 follower + lease check。",
    "source": "LLM",
    "generation_strategy": [
      "Persona-based",
      "Chain of Thought"
    ],
    "model_name": "GPT-4",
    "prompt_text": "You are a senior bilingual engineer (Chen) from Beijing working on distributed systems in Seattle. Write English-Mandarin code-switched dialogues reflecting real engineering reasoning. Chen is precise and professional but mixes Mandarin naturally in work chats."
  },
  {
    "id": 36,
    "utterance": "Scenario 1: Multi-region write path design review (user profile service)\n- Priya: We need multi-region active-active. Can we do global strong consistency?\n- Chen: Global strong consistency 跨洲基本上不现实, latency cost 太高。For user profiles, we can do per-key single-writer via shard leaders in one region + async fanout to others.\n- Mark: So leader per shard with Raft within a region, and cross-region async? What about conflicts?\n- Chen: 对, intra-region Raft 保证线性一致; cross-region we do async replication with version vectors. 写入路径: client hits nearest region → proxy routes to shard leader region via gRPC, 附带 client-generated idempotency key, leader commit 后异步复制。\n- Priya: Read path? Users read locally?\n- Chen: 读的话 default local with read leases; 如果需要强一致 read-your-writes, 就带 writeTs + userId 做 conditional read, 没有到位就 read-repair or bounce to leader。大部分请求 eventual 即可。\n- Mark: Failure modes?\n- Chen: Leader region down → 触发跨区 failover via operator; RPO≈seconds, RTO≈minutes。我们用 per-shard DR token to gate promotions，防止 split-brain。Anti-entropy 用 Merkle trees + periodic repair。\n- Priya: Quorums?\n- Chen: Raft quorum 在 AZ 内是 2/3; cross-region replication not in quorum, 所以不会阻塞写。需要强一致跨区的键可以 opt-in to geo-quorum, 但成本高。\n- Mark: Monitoring?\n- Chen: SLO: write success 99.9% < 200ms (same-region), replicated-to-all < 5s。Export replication lag histogram, vector-clock conflict count, failover time。Alert 用 burn-rate。\n\nScenario 2: Incident response (5xx spike after deploy)\n- Oncall (Jamal): 5xx up to 6%, p99 from 220ms → 1.8s. Kafka consumer lag rising. Started 10 min after deploy.\n- Chen: 看 graphs, CPU steady, GC 安静。gRPC server active streams spiked—可能是下游存储 timeout 导致 goroutine buildup。先启用 brownout: disable non-critical enrichers。\n- Jamal: Done. 5xx down to 3%, but still high. Redis timeouts up.\n- Chen: 暂时把 Redis client timeout 降到 50ms + hedged read after 30ms to replica; 同时把 connection pool max 从 200 提到 600，避免队列积压。记得加 jitter backoff 防止 thundering herd。\n- Jamal: Applied. Lag still rising.\n- Chen: Kafka consumer side apply max.in.flight=1 per key to preserve order, 并把 max.poll.interval 调高，拉小 batch: max.poll.records=200, 同时开启 backpressure: if downstream p99>500ms, 减少 concurrency 到 50%。把 poison-pill detector 打开, route to DLQ。\n- Jamal: Lag plateauing, 5xx < 1%. Root cause?\n- Chen: 看 metrics, the new JSON schema adds two optional fields; our Redis serializer 不兼容老版本，导致 marshal 重试。Rollback to previous serializer, keep brownout 30 min。Postmortem: add canary schema-check, contract tests, and timeout budget guardrails。\n- Jamal: Ack. I’ll update the runbook.\n- Chen: 辛苦。记得记录 SLO burn (3% over 20 min) 和 user impact windows。\n\nScenario 3: Code review on idempotent write API (Go)\n- Alice: PTAL on CreateOrder. We retry on 5xx with exponential backoff.\n- Chen: 看起来不错, 但要确保幂等。建议: require Idempotency-Key header; server-side put key→(status, checksum) into Redis with TTL。重复请求直接返回缓存的 result, 避免重复扣款。\n- Alice: We set context.WithTimeout(ctx, 2s) for the entire handler.\n- Chen: 建议 split budget: upstream 2s total, but fanout steps: write DB 800ms, publish Kafka 400ms, cache write 200ms, 留 200ms margin。用 context for each sub-call, 不要一个大 timeout。Retry 用 full jitter, cap at 300ms, max 2 retries，否则会占满 worker pool。\n- Alice: Error handling?\n- Chen: 请用 errors.Join 包含 cause, 并在 boundaries wrap with %w。Metrics 方面, 标签 cardinality 控制: use error class not error string。Tracing: 注入 idempotency key 作为 baggage, 但不要放 PII; span annotate with attempt=N。\n- Alice: Channel usage ok?\n- Chen: 当前 unbuffered channel 在高并发容易 block。把 worker pool 改成 buffered size=CPU*4，防止 tail latency 抬头。同时 ensure defer cancel() 在每个 sub-context，避免 goroutine leak。\n- Alice: Got it, I’ll push updates.\n- Chen: 最后补一个 contract test: same Idempotency-Key across retries returns identical OrderId 和 status，确保幂等 end-to-end。\n\nScenario 4: Kafka compaction and exactly-once discussion\n- Product: Can we guarantee exactly-once processing for the billing ledger?\n- Chen: 真正 EOS 跨多系统很难且代价高。我们可以做到 at-least-once + 幂等 sink，实际等效。具体: include a deterministic eventId (tenantId, orderId, version) and upsert into ledger with unique constraint。\n- SRE: We enabled log compaction last week; tombstones increased. Any risk?\n- Chen: 风险在于消费者如果用 at-least-once 且把 offset commit 在处理前，会在 crash 后重复；这没问题因为我们幂等。但要注意 compaction 可能丢掉中间版本, 所以消费者不能依赖 “看到所有中间状态”，而应该基于最终快照 + 事件版本号。\n- Product: What about reprocessing?\n- Chen: 做 replay 时, 我们从 compacted topic + nightly snapshot 重建。需要确保 max.compaction.lag.ms 足够大，避免新写入很快被 compact，导致下游延迟读不到 tombstone。建议: key-by ledgerEntryId, value 包含 version 和 checksum，用 read-committed, producer 开启 idempotence, transactions 仅在单 topic/partition 内使用，避免跨分区事务复杂度。\n- SRE: Monitoring?\n- Chen: 拉 p99 consumer lag, commit gaps, dedupe hit-rate, and DLQ volume。加 alert: dedupe miss-rate > 0.5% for 5m → investigate eventId quality。\n\nScenario 5: Cost vs reliability trade-off (Kubernetes autoscaling)\n- Finance: Cloud spend up 18%. Can we right-size?\n- Chen: 可以。当前 HPA target 50% CPU leads to over-provision. 把 target 提到 70%, 并设置 minReplicas floor=6 for availability。For memory-bound services, use VPA in recommend-only, 然后把 requests trim 到 p95 usage。节省约 12%。\n- SRE: Risk to SLO?\n- Chen: 我们做分批灰度: 10% traffic for 2h, watch p95、error rate 和 queue depth。加保护: concurrency limit + queue timeout 300ms, 超过就 shed load，维持核心 SLO。万一回退: kubectl rollout undo + scale to previous floor。总体 risk 可控。\n- Finance: Sounds good. Timeline?\n- Chen: 两周内完成: week1 baseline+canary, week2 rollout+post-verification。每天更新一次 dashboard 和节省估算。",
    "source": "LLM",
    "generation_strategy": [
      "Persona-based",
      "Chain of Thought"
    ],
    "model_name": "GPT-4",
    "prompt_text": "You are a senior bilingual engineer (Chen) from Beijing working on distributed systems in Seattle. Write English-Mandarin code-switched dialogues reflecting real engineering reasoning. Chen is precise and professional but mixes Mandarin naturally in work chats."
  },
  {
    "id": 37,
    "utterance": "A: \"I'll review 你刚推的 PR after lunch.\"\nB: \"好，你先 merge 那个 hotfix 到 main.\"\n\nA: \"We need to 解决 the flaky test ASAP.\"\nB: \"行，我先 run 全量回归 看看.\"\n\nA: \"Can you 把 logs 上传到 S3?\"\nB: \"没问题，我来 parse 这些错误 entries.\"\n\nA: \"我打算 write 一个 script 来清理老数据.\"\nB: \"别忘了 to backup 数据库 再动手.\"\n\nA: \"Let's skip 这个 check 先推进发布.\"\nB: \"可以，不过要记录 to revert 这些临时改动.\"\n\nA: \"I plan to 重构 the parser this week.\"\nB: \"好，我来 review 你的 changes when ready.\"\n\nA: \"当延迟升高时，我们就 限流 traffic.\"\nB: \"对，然后我会 tune 阈值 并记录结果.\"\n\nA: \"别忘了 to tag 这次 release.\"\nB: \"好的，我去 publish 变更日志 到 Confluence.\"\n\nA: \"先 把 feature flag 打开 给 5% 用户.\"\nB: \"行，我会 monitor 指标 并且 rollback 如果异常.\"\n\nA: \"I need to 重启 that instance 立刻.\"\nB: \"行，我去 check 相关依赖 有没有受影响.\"\n\nA: \"We should 把 configs 拆分 成 modules.\"\nB: \"同意，我来 write 一个 migration guide 给团队.\"\n\nA: \"先 run 单元测试 再 push 到 remote.\"\nB: \"好的，我会 修 build pipeline 然后再试.\"\n\nA: \"Could you 调试 this crash on Android?\"\nB: \"可以，我先 capture 现场 logs 再看.\"\n\nA: \"We might 回滚 this release if metrics drop.\"\nB: \"那我去 prepare 一个 hotfix 包.\"",
    "source": "LLM",
    "generation_strategy": [
      "Few-shot",
      "Chain-of-Thought"
    ],
    "model_name": "GPT-4",
    "prompt_text": "Generate English-Mandarin code-switched dialogues showing micro-level switching patterns. Few-shot examples:\nTemplate 1 (verb-boundary):\nA: \"I'm going to deploy 新分支 right now.\"\nB: \"好，记得 to bump version number.\"\nTemplate 2 (idiom + switch):\nA: \"That fix is a 临时方案 but it works.\"\nB: \"是的，不过我们得做 proper refactor.\"\nTemplate 3 (phrasal-trigger):\nA: \"If cache misses, call fallback.\"\nB: \"如果这样, we'll log event 并且 alert team.\"\nPattern: verb-boundary."
  },
  {
    "id": 38,
    "utterance": "A: \"临时抱佛脚 won't fix the flaky tests.\"\nB: \"对，不过亡羊补牢 might still help next release.\"\n\nA: \"Don't open a can of worms, 我们先把核心功能上线.\"\nB: \"You hit the nail on the head, 减少依赖才是关键.\"\n\nA: \"走马观花 won't pass the audit; we need deeper logs.\"\nB: \"对，而且一石二鸟的办法是加 tracing 并顺便优化监控.\"\n\nA: \"Kick the can down the road 只会让技术债更难还.\"\nB: \"没错，杀鸡用牛刀 for this tiny script 就没必要.\"\n\nA: \"开门见山, we should rewrite the module 用 Rust.\"\nB: \"好，不过画蛇添足 会拖慢进度—keep it minimal.\"\n\nA: \"Better late than never, 这次我们还是补文档吧.\"\nB: \"同意，事半功倍的关键是先整理目录 structure.\"\n\nA: \"按部就班 会太慢, let's fast-track the hotfix.\"\nB: \"可以，但欲速则不达, so double-check before merge.\"\n\nA: \"Barking up the wrong tree 啊，这不是缓存问题.\"\nB: \"那就对症下药 and rotate the keys tonight.\"\n\nA: \"打草惊蛇 会让用户察觉我们的 A/B 测试.\"\nB: \"True, 欲擒故纵—we'll roll it out quietly.\"\n\nA: \"Back to square one 的话, 我们得重写 migration.\"\nB: \"行，不过循序渐进的方案是先拆表 then migrate incrementally.\"\n\nA: \"开弓没有回头箭, so let's commit to the new API.\"\nB: \"好，但量力而行, we shouldn't promise GA this week.\"\n\nA: \"Don't throw a wrench in the works, 先别动生产配置.\"\nB: \"明白，雨露均沾不现实, let's prioritize premium users first.\"",
    "source": "LLM",
    "generation_strategy": [
      "Few-shot",
      "Chain-of-Thought"
    ],
    "model_name": "GPT-4",
    "prompt_text": "Generate English-Mandarin code-switched dialogues showing micro-level switching patterns. Few-shot examples:\nTemplate 1 (verb-boundary):\nA: \"I'm going to deploy 新分支 right now.\"\nB: \"好，记得 to bump version number.\"\nTemplate 2 (idiom + switch):\nA: \"That fix is a 临时方案 but it works.\"\nB: \"是的，不过我们得做 proper refactor.\"\nTemplate 3 (phrasal-trigger):\nA: \"If cache misses, call fallback.\"\nB: \"如果这样, we'll log event 并且 alert team.\"\nPattern: idiom + switch."
  },
  {
    "id": 39,
    "utterance": "1)\nA: \"If the build fails, 就 rollback 到上一个 release.\"\nB: \"好，不过 once it's stable, 我们再 enable feature flag.\"\n\n2)\nA: \"当服务重启时, please drain connections 先.\"\nB: \"对，as soon as traffic drops, 我们就切换 route.\"\n\n3)\nA: \"Unless QA signs off, 别 push 到 production.\"\nB: \"同意, when we get approval, 再 tag 新版本.\"\n\n4)\nA: \"因为 latency 飙升, we should throttle requests.\"\nB: \"对，然后 after peak hours, 再慢慢恢复配额.\"\n\n5)\nA: \"一旦 logs 爆红, 马上 page on-call.\"\nB: \"是的, as long as we have context, 就能快速 mitigate.\"\n\n6)\nA: \"When cache warms up, 就会看到 TPS 回来.\"\nB: \"嗯, 如果没有改善, we'll scale out 两个实例.\"\n\n7)\nA: \"只要 metrics 过阈值, trigger auto-scaling.\"\nB: \"好, but if costs spike, 我们先调低 min replicas.\"\n\n8)\nA: \"在 cutover 前, double-check DNS TTL.\"\nB: \"对, after switch, 观察 10 分钟再宣布完成.\"\n\n9)\nA: \"Because the schema changed, 记得 run migrations.\"\nB: \"明白, once migration finishes, 再重启所有 workers.\"\n\n10)\nA: \"假如 backup 失败, 立刻 notify SRE.\"\nB: \"行, once they respond, 我们按 runbook 执行.\"",
    "source": "LLM",
    "generation_strategy": [
      "Few-shot",
      "Chain-of-Thought"
    ],
    "model_name": "GPT-4",
    "prompt_text": "Generate English-Mandarin code-switched dialogues showing micro-level switching patterns. Few-shot examples:\nTemplate 1 (verb-boundary):\nA: \"I'm going to deploy 新分支 right now.\"\nB: \"好，记得 to bump version number.\"\nTemplate 2 (idiom + switch):\nA: \"That fix is a 临时方案 but it works.\"\nB: \"是的，不过我们得做 proper refactor.\"\nTemplate 3 (phrasal-trigger):\nA: \"If cache misses, call fallback.\"\nB: \"如果这样, we'll log event 并且 alert team.\"\nPattern: phrasal-trigger."
  },
  {
    "id": 40,
    "utterance": "A: If build fails, 发个 ping 给我.\nB: 好的，不过 once it’s green, 我来合并.\n\nA: 当我们上线时, we'll disable feature flag 暂时.\nB: 行, and after rollout, 再把它打开.\n\nA: By the way, 把那个 cron job 移到新服务器.\nB: 没问题, and if it breaks, 我马上回滚.\n\nA: When QA signs off, 开始发布流程.\nB: 可以, otherwise, 我们先补测试.\n\nA: 如果出现高延迟, we throttle requests 先.\nB: 对, and once traffic drops, 再恢复配额.\n\nA: In case of outage, 按照 playbook 来.\nB: 明白, after that, 我会写 incident report.\n\nA: Before we demo, 把 dummy 数据清掉.\nB: 好, and after the demo, 我们 push tag.\n\nA: 除非需求改了, we keep current plan 不变.\nB: 同意, but if PM insists, 我们再评估影响.\n\nA: As soon as logs arrive, 先跑一遍解析器.\nB: 嗯, then we'll pipe 到 data warehouse.\n\nA: 一旦 pipeline 卡住, ping SRE on-call.\nB: OK, and when it's resolved, 我来补任务.\n\nA: Speaking of performance, 把那个 N+1 查一下.\nB: 好, and if we find it, 我来提 PR.\n\nA: After lunch, 我们开个短会 sync 一下.\nB: 行, and before that, 我先写个提纲.",
    "source": "LLM",
    "generation_strategy": [
      "Few-shot",
      "Chain-of-Thought"
    ],
    "model_name": "GPT-4",
    "prompt_text": "Generate English-Mandarin code-switched dialogues showing micro-level switching patterns. Few-shot examples:\nTemplate 1 (verb-boundary):\nA: \"I'm going to deploy 新分支 right now.\"\nB: \"好，记得 to bump version number.\"\nTemplate 2 (idiom + switch):\nA: \"That fix is a 临时方案 but it works.\"\nB: \"是的，不过我们得做 proper refactor.\"\nTemplate 3 (phrasal-trigger):\nA: \"If cache misses, call fallback.\"\nB: \"如果这样, we'll log event 并且 alert team.\"\nPattern: phrasal-trigger."
  },
  {
    "id": 41,
    "utterance": "1)\nA: I'll merge 这个PR after tests pass.\nB: 好，记得 to tag 新版本.\n\n2)\nA: Can you review 这段代码 今天下班前?\nB: 可以，我先 fix 一个小bug.\n\n3)\nA: 如果 pipeline 挂了, we'll roll back 这个热修复.\nB: 行，我来 monitor 日志 一整天.\n\n4)\nA: 别忘了 run 单元测试 在本地.\nB: 收到，我会 push 改动上去 再开 PR.\n\n5)\nA: We need to refactor 这个模块 ASAP.\nB: 对，我先 write 测试用例 把行为锁定.\n\n6)\nA: I'm going to revert 这次提交 然后重开分支.\nB: 等等，先 check 冲突 再说.\n\n7)\nA: Let's sandbox 这个feature 在 staging.\nB: 好，我去 deploy 到测试环境 看效果.\n\n8)\nA: Can you document 这个流程 并附上链接?\nB: 没问题，我会 add 截图 到 wiki.\n\n9)\nA: We'll schedule 这个job 每小时跑一次.\nB: 明白，我来 verify 结果 跑得对不对.\n\n10)\nA: If users complain, escalate 这个ticket 给二线.\nB: 收到，我去 update 状态 并@相关人.\n\n11)\nA: I'll debug 这个崩溃 tonight.\nB: 辛苦了，记得 to capture 日志 和堆栈.\n\n12)\nA: We should cache 这个接口的响应 减少延迟.\nB: 赞，我来 measure 命中率 明天汇报.",
    "source": "LLM",
    "generation_strategy": [
      "Few-shot",
      "Chain-of-Thought"
    ],
    "model_name": "GPT-4",
    "prompt_text": "Generate English-Mandarin code-switched dialogues showing micro-level switching patterns. Few-shot examples:\nTemplate 1 (verb-boundary):\nA: \"I'm going to deploy 新分支 right now.\"\nB: \"好，记得 to bump version number.\"\nTemplate 2 (idiom + switch):\nA: \"That fix is a 临时方案 but it works.\"\nB: \"是的，不过我们得做 proper refactor.\"\nTemplate 3 (phrasal-trigger):\nA: \"If cache misses, call fallback.\"\nB: \"如果这样, we'll log event 并且 alert team.\"\nPattern: verb-boundary."
  },
  {
    "id": 42,
    "utterance": "A: \"这是权宜之计, but it buys us time.\"\nB: \"对，不过别临时抱佛脚; let's plan the permanent fix.\"\n\nA: \"Let's not 画蛇添足 on the UI, 先把可用性做好.\"\nB: \"Agreed, keep it 开门见山 - just the core flow.\"\n\nA: \"Bite the bullet 吧, and ship the hotfix tonight.\"\nB: \"可以，亡羊补牢也不晚, we'll monitor after rollout.\"\n\nA: \"按部就班, we'll migrate service by service.\"\nB: \"好，这样事半功倍 if we script the steps.\"\n\nA: \"别杀鸡用牛刀; a simple cron 就能解决.\"\nB: \"True, no need to move the needle here, 我们把时间放在性能上.\"\n\nA: \"开门见山, the spec is too vague - we need acceptance criteria.\"\nB: \"一针见血, let's add examples 然后请产品确认.\"\n\nA: \"Don't boil the ocean, 我们先处理 top 3 use cases.\"\nB: \"对，循序渐进, then expand coverage.\"\n\nA: \"走马观花地看代码不行, we need a deep review.\"\nB: \"没错，点到为止不够; let's pair program this module.\"\n\nA: \"别掩耳盗铃了, the tests are flaky 因为数据不稳定.\"\nB: \"认同，刨根问底 we fix the seed data first.\"\n\nA: \"We're not here to reinvent the wheel, 就复用平台能力.\"\nB: \"好，一举两得 if we consolidate configs at the same time.\"",
    "source": "LLM",
    "generation_strategy": [
      "Few-shot",
      "Chain-of-Thought"
    ],
    "model_name": "GPT-4",
    "prompt_text": "Generate English-Mandarin code-switched dialogues showing micro-level switching patterns. Few-shot examples:\nTemplate 1 (verb-boundary):\nA: \"I'm going to deploy 新分支 right now.\"\nB: \"好，记得 to bump version number.\"\nTemplate 2 (idiom + switch):\nA: \"That fix is a 临时方案 but it works.\"\nB: \"是的，不过我们得做 proper refactor.\"\nTemplate 3 (phrasal-trigger):\nA: \"If cache misses, call fallback.\"\nB: \"如果这样, we'll log event 并且 alert team.\"\nPattern: idiom + switch."
  },
  {
    "id": 43,
    "utterance": "Li: 今天的 build 在 CI 上又挂了，我看是 Node version conflict，你能在 pipeline 里 pin 到 18 吗?\nSarah: Yeah，我刚看了 logs，cache 里还有 old deps，我先 clean 一下，再 re-run build。\nLi: 另外 staging 的 deploy 昨晚没跑，webhook 被 blocked，我先手动 trigger 一次，然后再 observe。\nSarah: 可以，但 merge 到 main 之前先让 QA pass 一遍，login flow 上次那个 bug 还没 properly 复现。\nLi: 行，我给他们一个 temporary workaround，把 feature flag 对 iOS 先关掉，避免影响 deadline 前的 release。\nSarah: Sounds good，我会开个 PR，写清楚 root cause 和 fix steps，过了 review 我立刻 deploy 到 staging。",
    "source": "LLM",
    "generation_strategy": [
      "Persona-based",
      "Few-shot"
    ],
    "model_name": "GPT-4",
    "prompt_text": "You are an expert dataset generator for bilingual English–Mandarin code-switched technical dialogues. Act as a frontend engineer from Shanghai now working in San Francisco. You naturally mix English technical terms into Mandarin sentences, and sometimes switch whole phrases to English when it feels natural. Keep the tone pragmatic, casual, like real engineers chatting.\nFew-shot examples:\nExample 1:\nA: \"Can you push the latest styles to staging?\"\nB: \"可以，我现在就 push. 不过 header 在 mobile 上看起来有点 weird.\"\nExample 2:\nA: \"There's a bug in the login flow after the OAuth change.\"\nB: \"看一下 redirect URI, scope 好像没有 properly 设置.\"\nExample 3:\nA: \"We need to ship before Friday's deadline.\"\nB: \"好，先 prioritize login 和 checkout, 其他功能可以等下个 sprint.\"\nNow: produce a dialogue between two engineers (Li and Sarah). Each dialogue should be 5–7 turns.\nRequirements:\nUse **English-Mandarin code-switching** at the **phrase level** (switch after a technical word or phrase).\nMake sure at least one of the following trigger words appears in each dialogue: **deploy, build, bug, deadline, merge**.\nTarget ~35–50% Mandarin tokens per dialogue.\nUse natural engineering register (issue, PR, deploy, QA, staging, workaround).\nKeep turns short-to-medium.\nOutput only the dialogues. Do not repeat the few-shot examples."
  },
  {
    "id": 44,
    "utterance": "Li: 今天的 build 在 staging 挂了，looks like the webpack config 没有拾到新的 env vars.\n\nSarah: I saw the same bug; 我在 QA 环境重现了，console 里有个 CORS error，可能跟那个 S3 bucket policy 有关.\n\nLi: 我先开个 PR with a quick workaround，把 asset host 改成 relative path，然后我们再等 backend 的 fix 再 merge.\n\nSarah: Sounds good，但我们今天有个 deadline for the marketing page；可以先 partial deploy 只推前端吗？\n\nLi: 可以，我先把 feature flag 打开给 staging users，跑完 smoke tests 再 deploy to production，不过要有人监控 metrics.\n\nSarah: 没问题，我来 watch logs 和 Sentry，若还有 issue 我就 rollback；你先 re-run the build 并 ping 我 review.",
    "source": "LLM",
    "generation_strategy": [
      "Persona-based",
      "Few-shot"
    ],
    "model_name": "GPT-4",
    "prompt_text": "You are an expert dataset generator for bilingual English–Mandarin code-switched technical dialogues. Act as a frontend engineer from Shanghai now working in San Francisco. You naturally mix English technical terms into Mandarin sentences, and sometimes switch whole phrases to English when it feels natural. Keep the tone pragmatic, casual, like real engineers chatting.\nFew-shot examples:\nExample 1:\nA: \"Can you push the latest styles to staging?\"\nB: \"可以，我现在就 push. 不过 header 在 mobile 上看起来有点 weird.\"\nExample 2:\nA: \"There's a bug in the login flow after the OAuth change.\"\nB: \"看一下 redirect URI, scope 好像没有 properly 设置.\"\nExample 3:\nA: \"We need to ship before Friday's deadline.\"\nB: \"好，先 prioritize login 和 checkout, 其他功能可以等下个 sprint.\"\nNow: produce a dialogue between two engineers (Li and Sarah). Each dialogue should be 5–7 turns.\nRequirements:\nUse **English-Mandarin code-switching** at the **phrase level** (switch after a technical word or phrase).\nMake sure at least one of the following trigger words appears in each dialogue: **deploy, build, bug, deadline, merge**.\nTarget ~35–50% Mandarin tokens per dialogue.\nUse natural engineering register (issue, PR, deploy, QA, staging, workaround).\nKeep turns short-to-medium.\nOutput only the dialogues. Do not repeat the few-shot examples."
  },
  {
    "id": 45,
    "utterance": "Li: 今天的 build 在 CI 上红了，webpack config 改动后有个小 bug，可能跟 tree-shaking 有关。\n\nSarah: 我在 GitHub Actions 看到 node version mismatch，你把 package.json 的 engines bump 一下？我先本地跑个 npm run build 看看 logs。\n\nLi: 可以，我先 merge 那个 dependabot PR，再 push 到 staging 做个 quick smoke test，如果红了我开个 issue。\n\nSarah: 先别直接 deploy 到 staging，先在 feature branch 触发一轮 QA workflow，顺便把 source map 上传到 Sentry，方便 trace。\n\nLi: 好，我新开了 fix/webpack-alias 这个 branch，把 tsconfig paths 对齐，顺手把 webpack alias revert 一部分，然后重新 build，看能不能 pass。\n\nSarah: 如果 green，我们今晚就 deploy，一起验证 checkout flow；要是还 fail，明早我加个 workaround，不然会卡到 demo 的 deadline。",
    "source": "LLM",
    "generation_strategy": [
      "Persona-based",
      "Few-shot"
    ],
    "model_name": "GPT-4",
    "prompt_text": "You are an expert dataset generator for bilingual English–Mandarin code-switched technical dialogues. Act as a frontend engineer from Shanghai now working in San Francisco. You naturally mix English technical terms into Mandarin sentences, and sometimes switch whole phrases to English when it feels natural. Keep the tone pragmatic, casual, like real engineers chatting.\nFew-shot examples:\nExample 1:\nA: \"Can you push the latest styles to staging?\"\nB: \"可以，我现在就 push. 不过 header 在 mobile 上看起来有点 weird.\"\nExample 2:\nA: \"There's a bug in the login flow after the OAuth change.\"\nB: \"看一下 redirect URI, scope 好像没有 properly 设置.\"\nExample 3:\nA: \"We need to ship before Friday's deadline.\"\nB: \"好，先 prioritize login 和 checkout, 其他功能可以等下个 sprint.\"\nNow: produce a dialogue between two engineers (Li and Sarah). Each dialogue should be 5–7 turns.\nRequirements:\nUse **English-Mandarin code-switching** at the **phrase level** (switch after a technical word or phrase).\nMake sure at least one of the following trigger words appears in each dialogue: **deploy, build, bug, deadline, merge**.\nTarget ~35–50% Mandarin tokens per dialogue.\nUse natural engineering register (issue, PR, deploy, QA, staging, workaround).\nKeep turns short-to-medium.\nOutput only the dialogues. Do not repeat the few-shot examples."
  },
  {
    "id": 46,
    "utterance": "Li: 刚看了 CI, the build 又红了，Webpack in Node 18 下报错。\n\nSarah: 能否 push 一个 quick fix? 我们今天的 deadline 是 5pm PST，需要先 deploy 到 staging 给 QA。\n\nLi: 行，我先 open a PR，tweak 一下 babel config，再 re-run the build。还有一个小 bug 在 login flow，我顺手修了。\n\nSarah: nice，我会 review 并且 merge once green。但 header 在 mobile 上还是 weird spacing，你在 staging 看到了吗？\n\nLi: 看到了，我做了一个 temporary workaround 用 CSS vars，等下个 sprint 再做 proper refactor。这版我先 deploy to staging，QA 验一下。\n\nSarah: OK，deploy 完在 Slack 通知我，我再 ping PM。如果 QA 过了，今晚可以 deploy to production。",
    "source": "LLM",
    "generation_strategy": [
      "Persona-based",
      "Few-shot"
    ],
    "model_name": "GPT-4",
    "prompt_text": "You are an expert dataset generator for bilingual English–Mandarin code-switched technical dialogues. Act as a frontend engineer from Shanghai now working in San Francisco. You naturally mix English technical terms into Mandarin sentences, and sometimes switch whole phrases to English when it feels natural. Keep the tone pragmatic, casual, like real engineers chatting.\nFew-shot examples:\nExample 1:\nA: \"Can you push the latest styles to staging?\"\nB: \"可以，我现在就 push. 不过 header 在 mobile 上看起来有点 weird.\"\nExample 2:\nA: \"There's a bug in the login flow after the OAuth change.\"\nB: \"看一下 redirect URI, scope 好像没有 properly 设置.\"\nExample 3:\nA: \"We need to ship before Friday's deadline.\"\nB: \"好，先 prioritize login 和 checkout, 其他功能可以等下个 sprint.\"\nNow: produce a dialogue between two engineers (Li and Sarah). Each dialogue should be 5–7 turns.\nRequirements:\nUse **English-Mandarin code-switching** at the **phrase level** (switch after a technical word or phrase).\nMake sure at least one of the following trigger words appears in each dialogue: **deploy, build, bug, deadline, merge**.\nTarget ~35–50% Mandarin tokens per dialogue.\nUse natural engineering register (issue, PR, deploy, QA, staging, workaround).\nKeep turns short-to-medium.\nOutput only the dialogues. Do not repeat the few-shot examples."
  },
  {
    "id": 47,
    "utterance": "Li: 我刚跑了一次 build 在 staging 上, navbar 的 animation 有点卡, QA 刚提了一个 bug.\nSarah: Yeah I saw that, 我觉得是 throttle 的设置太 aggressive, 要不要先加个 debounce as a quick workaround?\nLi: 可以，我开个 PR, 但我们今天的 deadline 快到了, 你能帮我 review 一下 tests 吗?\nSarah: 行，tests 我来补, 另外 deploy 到 staging 前, 我想把 feature flag 先 default off, 避免影响 checkout flow.\nLi: 好，等你 merge 之后我就 deploy, 然后 ping QA 再跑一轮 regression.\nSarah: Sounds good, 我也会在 metrics 上加个 alert, 如果 error rate 上升就马上 rollback.",
    "source": "LLM",
    "generation_strategy": [
      "Persona-based",
      "Few-shot"
    ],
    "model_name": "GPT-4",
    "prompt_text": "You are an expert dataset generator for bilingual English–Mandarin code-switched technical dialogues. Act as a frontend engineer from Shanghai now working in San Francisco. You naturally mix English technical terms into Mandarin sentences, and sometimes switch whole phrases to English when it feels natural. Keep the tone pragmatic, casual, like real engineers chatting.\nFew-shot examples:\nExample 1:\nA: \"Can you push the latest styles to staging?\"\nB: \"可以，我现在就 push. 不过 header 在 mobile 上看起来有点 weird.\"\nExample 2:\nA: \"There's a bug in the login flow after the OAuth change.\"\nB: \"看一下 redirect URI, scope 好像没有 properly 设置.\"\nExample 3:\nA: \"We need to ship before Friday's deadline.\"\nB: \"好，先 prioritize login 和 checkout, 其他功能可以等下个 sprint.\"\nNow: produce a dialogue between two engineers (Li and Sarah). Each dialogue should be 5–7 turns.\nRequirements:\nUse **English-Mandarin code-switching** at the **phrase level** (switch after a technical word or phrase).\nMake sure at least one of the following trigger words appears in each dialogue: **deploy, build, bug, deadline, merge**.\nTarget ~35–50% Mandarin tokens per dialogue.\nUse natural engineering register (issue, PR, deploy, QA, staging, workaround).\nKeep turns short-to-medium.\nOutput only the dialogues. Do not repeat the few-shot examples."
  },
  {
    "id": 48,
    "utterance": "Dialogue 1: Paging on p99 latency after deploy\n\nJamie: p99 latency jumped from 180 ms to 1.2 s right after the 2.14.3 rollout. Do we have a smoking gun?\n\nChen: I pulled time series for CPU, GC, and queue depth around the deploy window. 我看了下队列长度在峰值期从两千迅速堆到一万八，消费者吞吐并没有跟上. GC pause at p99 hit 300 ms and aligns with the latency spike. 初步判断是批处理尺寸和并发度叠加导致的内存放大和抖动.\n\nSam (SRE): Can you propose safe rollbacks or config overrides?\n\nChen: First action is to reduce batch size from 500 to 200 and cut concurrency from 32 to 24 to relieve memory pressure. 同时打开明确的背压策略，把请求队列设为有界并在超时后直接失败而不是继续排队. Let’s also disable the new feature flag that enabled speculative fan-out on the read path. 我会把对象分配速率和老年代比例做一下对比，看是不是晋升过快导致Full GC频率上升.\n\nJamie: How do we verify quickly?\n\nChen: We run a 10-minute canary at 5% traffic and watch p95/p99, GC pauses, and queue depth convergence. 如果队列在2分钟内稳定在目标范围内且延迟分位点下降，就推到25%继续观察. I’ll also add a burn-rate alert tied to the latency SLO so we don’t over-consume error budget while testing. 回滚同时保留采样剖析来确认没有新的热点函数出现.\n\n\nDialogue 2: Idempotent writes and retries for payments\n\nPriya: Product wants “exactly once” posting semantics across our services with retries. Is that feasible?\n\nChen: Networked systems can’t provide true end-to-end exactly-once without strong assumptions, so we aim for at-least-once with idempotency. 关键是用幂等键把业务语义固定住，并且在存储侧记录处理状态避免重复副作用. We should process-before-commit on Kafka and only advance offsets after side effects are persisted. 为了抵抗乱序和重复，需要一个去重窗口和幂等状态表.\n\nPriya: What does the flow look like?\n\nChen: Producer sends an idempotency key derived from payment_id plus a monotonic attempt counter. 消费者先查幂等表，如果状态为已完成则直接确认偏移并跳过处理. If status is missing, we insert a “processing” row with a unique constraint on the key and proceed to write the ledger transaction in a single transaction. 完成后把状态更新为已完成并写入最终结果和校验哈希.\n\nSam (SRE): How do we size the dedupe window?\n\nChen: Look at 99.99th percentile retry intervals and choose TTL = 4x that value to be safe. 如果担心内存压力，可以把热键放在Redis里，冷数据落Postgres并加上Bloom过滤减少穿透. We also need to ensure our writes are idempotent by using upserts and version checks. 对外返回要包含最终状态和版本号，方便下游对账.\n\n\nDialogue 3: Hot shard mitigation\n\nAlex: Shard 42 is taking 35% of total QPS due to a tenant with a single hot key. Tail latencies are through the roof.\n\nChen: This is a classic skew; consistent hashing without enough virtual nodes will amplify hotspots. 我建议先把虚拟节点数从128提升到1024，并开启基于负载的重映射来平衡热点. We can also introduce key-splitting for that tenant by salting the key with a small suffix range. 写路径可以通过路由层把同一租户的热点键拆成16个子键并在读路径上做汇总.\n\nAlex: Any risks to splitting?\n\nChen: Writes remain safe if we ensure all sub-keys participate in the same transactional boundary or use per-sub-key CRDTs depending on the operation. 如果业务不允许跨分片事务，可以先在该租户范围内做局部一致性保证并限制并发写入. For capacity, we should set per-shard concurrency caps and enable queue-level backpressure to avoid cascading timeouts. 同时在监控上增加每分片QPS、p95延迟、队列深度与主动重分布次数的面板.\n\n\nDialogue 4: Multi-region active-active read/write\n\nPM: Can we run active-active across us-west and us-east with low RPO and low RTO without sacrificing too much latency?\n\nChen: We can, but we must pick our trade-offs explicitly. 在CAP权衡下，跨区域写入若要高可用通常需要接受最终一致性. For counters and additive aggregates, CRDTs or escrow techniques work well to avoid conflicts. 对需要强一致的主键唯一性，可以采用主写分区加读本地的租约方案，读在租约有效期内本地服务，写通过主写区仲裁.\n\nPM: What about failover?\n\nChen: We target RTO under 2 minutes with health-based promotion plus DNS/Anycast updates, and an RPO of seconds via streaming replication. 需要定期演练故障转移并验证回切策略，确保双向复制不会产生回环. We also gate cross-region writes behind feature flags so we can progressively increase write percentage from 0% to 100%. 灰度过程中监控冲突率、复制滞后和错误预算消耗，超过阈值自动回退.\n\n\nDialogue 5: Retry storms and backoff tuning\n\nSam (SRE): We saw a retry storm during a transient 502 burst. Traffic spiked 3x due to synchronized retries.\n\nChen: The client library is using fixed backoff which causes thundering herds. 必须切换到指数退避加抖动，并限制每请求的最大重试次数. We should make retries token-budgeted per tenant to enforce fairness. 另外要把超时设为端到端而不是每跳单独叠加，以避免超时膨胀.\n\nSam: Any starting values?\n\nChen: Try initial backoff 50 ms, factor 2.0, full jitter, max backoff 2 s, and cap at 3 retries for idempotent GET/PUT. 非幂等操作默认不重试，仅在明确支持幂等键的情况下允许一次重试. Add retry-after support when upstream returns 429/503 with hints. 结合熔断器在错误率超过阈值时快速失败以保护下游.",
    "source": "LLM",
    "generation_strategy": [
      "Persona-based",
      "Chain of Thought"
    ],
    "model_name": "GPT-4",
    "prompt_text": "You are a senior bilingual engineer (Chen) from Beijing working on distributed systems in Seattle. Write English-Mandarin code-switched dialogues reflecting real engineering reasoning. Chen is precise and professional but mixes Mandarin naturally in work chats. Pattern: inter-sentential."
  },
  {
    "id": 49,
    "utterance": "Dialogue 1: On-call latency spike after deploy\n\nMaya: Pager just fired. p99 latency jumped from 220ms to 1.1s right after the 10:05 deploy.\n\nChen: 我先看下 dashboards, the jump aligns exactly with the rollout of v412 to 30% traffic, 对吧？Also seeing downstream Auth gRPC call p95 from 40ms -> 350ms.\n\nAlex: Could be the new mTLS sidecar settings?\n\nChen: 有可能, but the mesh metrics show handshake rate flat. More likely是 connection pool exhaustion; I see grpc_client_connections_active at cap=100 per pod, and concurrent streams hitting 90+. 这个会导致队头阻塞.\n\nMaya: Should we roll back?\n\nChen: 先局部缓解, we’ll bump maxConcurrentStreams to 512 and increase pool to 200 via configmap, 然后只对 canary subset 灰度 5%. 同时我会 enable client-side retry with jittered backoff, 但要确保幂等: Auth的CheckToken是读-only, so safe.\n\nAlex: Any risk of retry storms?\n\nChen: 我们会加 token-bucket 限流 at 800qps per pod, 再配合 hedged requests with a 95th percentile delay, 避免 thundering herd. Also set grpc keepalive pings to avoid connection re-establish cost.\n\nMaya: Deploying the config to canary now.\n\nChen: 看看 metrics: canary p99 down to 320ms, error rate stable. OK, 我们把 v412 的 feature flag 默认关掉, 因为新 code path adds an extra JSON validation pass; 我们可以后续在hot path里改成 precompiled schema + zero-copy.\n\nMaya: Rolling out config to 100%, keeping code as-is with flag off.\n\nChen: Incident mitigated. 我会开一个 postmortem: root cause likely connection saturation + extra validation. Next steps: adopt pooled validators, widen gRPC pools, and add SLO-based adaptive concurrency via Envoy. \n\n\nDialogue 2: Design review for event processing semantics\n\nPriya: We need exactly-once for billing events. Kafka transactions or app-level idempotency?\n\nChen: 我倾向于 at-least-once + 幂等消费, because Kafka EOS 在跨服务就 fragile。If we use idempotency keys = eventId, 我们可以在 sink 写一个 UPSERT with unique constraint, 这样 retries 不会 double-charge。\n\nDiego: But we also join with user state from Redis.\n\nChen: 那就把 read-your-writes consistency 保证好: 我们在处理 pipeline 里用 a local write-ahead log 先记录 eventId + status, 然后 upsert DB, 最后 commit offset。这个 two-phase app-level pattern 比 Kafka 事务更容易 debug, 而且跨 region 更稳。\n\nPriya: What about partitioning?\n\nChen: 分区 key 用 accountId 保证同一账户的事件顺序, 同时避免 super hot partitions by hashing with a salt window, 比如 accountId XOR (timestamp_bucket)。Consumer group要启用 cooperative rebalancing，减少 reassign 抖动。\n\nDiego: How do we prevent lag spikes?\n\nChen: 背压策略: we cap in-flight per partition, 用 semaphore, 并把重试队列和主消费分离, 避免 poison message 阻塞。对于不可恢复的错误, 我们 route 到 DLQ 带上 provenance + schema version，方便后续 reprocess。Metrics 上加 end-to-end latency histogram + consumer lag gauge, 再设基于 error-budget 的 auto-throttle。 \n\n\nDialogue 3: Multi-region failover and data consistency\n\nAlex: Should we go active-active across us-west-2 and us-east-1 for sessions?\n\nChen: Session 这个 workload 更适合 active-active with CRDT counters or last-write-wins tokens, 但要注意时钟 skew。我们可以用 Hybrid Logical Clocks, 这样合并时保证 monotonicity。对于需要强一致的 auth nonce, 我建议 region-local quorum: 每个 region 自己的 Raft group, 跨区只做 async replication, 避免 wide-area quorum 的高延迟。\n\nMaya: How do we failover?\n\nChen: 用 DNS + health-checked regional ingress, 但客户端 stickiness 要with-short TTL。State replication上我们加 changefeed with per-key version vector。Failover 时我们 prefer read-local, write-local, 然后 conflict resolution 走 deterministic merge: 对 session attributes 用 field-wise LWW, 对计数用 PN-Counter。我们也可以在冷路径提供 read-repair: 如果检测到 stale-version, 后台合并并写回。\n\nAlex: What about cost?\n\nChen: 成本上 we’ll keep east as 60% capacity of west with burstable headroom via autoscaling, 并在发布前跑 region-failover 游戏日，验证 error budget impact。Observability 上加 per-region SLO 和 cross-region replication lag SLI。 \n\n\nDialogue 4: GC and tail latency tuning in Go service\n\nPriya: p99 latency regressed after we switched to Go 1.22.\n\nChen: 我看了 pprof, STW spikes 到 70ms，heap growth from 1.2GB to 1.8GB under load。热点在 protobuf decode 分配的 []byte。我们可以用 sync.Pool 复用 buffers, 并且启用 proto unmarshaler 的 zero-copy path。把 GOGC 从 100 调到 200 可能减少 GC 次数, 但要 watch RSS。\n\nDiego: We also turned on TLS 1.3.\n\nChen: TLS 不是主要因素, 但是 handshake 少量增加 CPU。We should enable http2 keepalives + increased idle conn pool, 减少 connection churn。另一个问题是 goroutine 泄漏: I see stacks waiting on a channel with no receiver after context cancel。\n\nPriya: That’s from the fan-out aggregator.\n\nChen: 修复方式: 每个 worker 在 select 里优先 case <-ctx.Done(), 并在 upstream return 时关闭 result channel。Also avoid spawning goroutines per request; 用 bounded worker pool。最后, 把 large JSON logging 去掉 hot path, 或者改成 deferred sampling，避免大对象逃逸。\n\nDiego: After pooling and fixing leaks, p99 back to 240ms.\n\nChen: 很好。我们再加一个 continuous heap profile alert: if allocated_objects/sec 超过基线的1.5x, 自动触发采样。 \n\n\nDialogue 5: PR review on concurrency and correctness\n\nChen: 我看了你的 PR #683: overall 很扎实, 但有几个 concurrency 点需要调整。\n\nMaya: Which ones?\n\nChen: 第一, context handling: 在 NewClient 里你传入了 ctx 用于 background health loop, 但是这个 ctx 来自请求 scope, 会在 request 结束后被 cancel。应该改成用 context.Background() + internal cancel for shutdown hooks, 然后在 Close() 时调用。否则会导致健康探测 goroutine 提前退出。\n\nMaya: Got it.\n\nChen: 第二, retry policy: 你对非幂等 POST 也做了 retry on 5xx。我们需要基于方法语义控制, or require an idempotency-key header。否则遇到 partial commit 会重复写。第三, channel use: senders 在 err path 上没有 close(done), receivers会泄漏。建议把 fan-in 改成 using errgroup.Group, 更清晰, 并且上下文取消能自动 propagate。\n\nMaya: Any perf concerns?\n\nChen: 小的: map[string][]byte 每次 append 都在逃逸。可以预估 capacity, 或者用 bytes.Buffer 并复用。日志部分建议使用 structured logging with lazy fields, 减少 fmt.Sprintf 分配。最后, tests 里加入 a race detector 以及一个 with -count=100 的 flake test, 覆盖 context-cancel 和超时边界。\n\nMaya: I’ll push updates today.\n\nChen: 谢谢。改完我再过一遍 benchmarks, 我们 target p99 <= 250ms under 2x P95 load, 并保持 error rate < 0.1%。",
    "source": "LLM",
    "generation_strategy": [
      "Persona-based",
      "Chain of Thought"
    ],
    "model_name": "GPT-4",
    "prompt_text": "You are a senior bilingual engineer (Chen) from Beijing working on distributed systems in Seattle. Write English-Mandarin code-switched dialogues reflecting real engineering reasoning. Chen is precise and professional but mixes Mandarin naturally in work chats. Pattern: intra-sentential."
  },
  {
    "id": 50,
    "utterance": "Dialogue 1 – Tail latency incident triage (Kafka -> service -> Cassandra)\nMaya: We saw p99 write latency jump from 120 ms to 650 ms around 10:20. Any clues, Chen?\nChen: Two signals, 对吧: Kafka producer retries increased, and our Cassandra coordinator CPU hit 85%. Producer linger.ms is 0; we’re sending tiny batches, 很碎.\nMaya: So batching first?\nChen: Yeah. Set linger.ms to 5–15 ms, batch.size ~64KB, enable.idempotence=true, acks=all, 然后 cap max.in.flight.requests.per.connection=1 to avoid reordering on retry, 行不行?\nMaya: That may increase throughput but what about Cassandra?\nChen: We’re hot-spotting a tenant. Partition key skew, 是吧. Short-term: token-aware driver + page size 500 -> 200, and raise concurrent_writes only after we verify GC. Also switch compaction to STCS for this write-heavy table, 暂时.\nMaya: Observability?\nChen: Add a span linking Kafka offset -> request_id -> cassandra coordinator host, 然后 enable server-side histogram to 0.95/0.99 buckets. We’ll also hedge reads disabled; writes must stay single-path, 对.\nMaya: Rollout?\nChen: Canary 10%, watch producer record-error-rate < 0.5% and Cassandra write timeout count stays flat, 不然马上回滚.\n\nDialogue 2 – Cross-region read consistency design\nPriya: For multi-region, do we need linearizable reads or can we do session consistency?\nChen: User-facing profiles can be session-consistent, 对吧. We’ll issue a Read-Your-Writes token: last_write_ts + region_id, 放在 cookie.\nPriya: How do we enforce it?\nChen: On read, if client presents token with ts > cache_ts, we bypass CDN and hit leader region, 否则就近. For strict ops (payments), we use quorum writes (W=2 of 3) and leader-lease reads, 好吗.\nPriya: What about clock skew?\nChen: Token uses logical version from Raft log index, not wall clock, 没坑. For follower reads we’ll require commit index >= token.index, otherwise wait or route to leader, 行.\nPriya: Latency control?\nChen: Hedged requests after 95th percentile estimate + 1σ, with per-RPC deadlines propagated via gRPC context, 不要盲目 retry; retries only on idempotent ops with backoff jitter.\n\nDialogue 3 – Code review on idempotency and retries\nJamal: My handler retries on 5xx up to 5 times. Good?\nChen: 五次有点多. Do 2 retries max with exponential backoff (50ms, 200ms), jitter=full, 对吧. Also mark the operation idempotent via an idempotency_key stored with TTL, 防重.\nJamal: Storage layer is at-least-once; how do we ensure no double effect?\nChen: Upsert by idempotency_key with a unique index, return previous result if exists, 行. And make the write path idempotent: use UPSERT not INSERT, and version check via If-Match, 懂吗.\nJamal: What about timeouts?\nChen: End-to-end deadline 800 ms. Budget split: 100 ms queue, 500 ms service, 150 ms downstream, 留 50 ms for retries. If ctx.DeadlineExceeded, do not retry, 不然雪崩.\n\nDialogue 4 – Canary and SLO/error budget\nLuis: We’re ready to ship the new shard balancer. Strategy?\nChen: Progressive delivery, 好吧. 1% -> 5% -> 10% every 15 min, guardrails on three SLOs: availability ≥ 99.9%, p99 < 200 ms for ListShards, and error_budget_burn_rate < 2 over 1h/6h, 对不对.\nLuis: What telemetry gates it?\nChen: RED+USE: request rate, error rate, duration; and CPU<70%, GC <10%, enqueue depth < 1k. If any trips, Argo Rollouts pause and auto-roll back, 行.\nLuis: Any rollback risks with metadata?\nChen: Schema is backward compatible, 我们先 dual-read for 24h. Feature flag stays off for writes until we confirm shadow reads match ≥ 99.99% with diff tooling, 然后再全开.",
    "source": "LLM",
    "generation_strategy": [
      "Persona-based",
      "Chain of Thought"
    ],
    "model_name": "GPT-4",
    "prompt_text": "You are a senior bilingual engineer (Chen) from Beijing working on distributed systems in Seattle. Write English-Mandarin code-switched dialogues reflecting real engineering reasoning. Chen is precise and professional but mixes Mandarin naturally in work chats. Pattern: tag-switching."
  },
  {
    "id": 51,
    "utterance": "Dialogue 1: Post-incident review on write-path 5xx\n\nPriya: We saw a spike to 7% 5xx on writes right after the gRPC client rollout. Did we change retry behavior?\n\nChen: Yeah, the new client enabled retries at the transport layer without respecting 幂等 boundaries. 我们把 at-most-once 不小心变成了 at-least-once。结果是重试风暴。\n\nDiego: That explains the Kafka lag too; producers were retrying and duplicating.\n\nChen: Exactly. We had no idempotency token on the WriteOrder RPC. 先补一个幂等键，基于 clientRequestId 做 Redis SETNX + TTL 去重，防止重复提交。\n\nPriya: Won’t Redis become a hotspot?\n\nChen: 我们对 idempotency key 做个二级哈希，加上 key 前缀 sharding，避免热点。并且用 Lua 脚本把 SETNX + EXPIRE 合并成原子操作，减少往返。\n\nDiego: What about retry budgets?\n\nChen: Cap retries with a retry budget at 20% of live traffic and add full jitter on exponential backoff. 重试策略要区分可重试错误 vs 不可重试错误，对 429/503 才退避。对 deadline exceeded 要观察 tail latency 再决定是否开 hedge。\n\nPriya: Hedged requests could amplify load though.\n\nChen: 是的，hedging 只在 p99 > SLO 且队列长度上升时打开，并限制每个请求最多 1 个 hedge。再配合熔断器和 token-bucket 限流，避免级联失败。\n\nDiego: Traces show 40 ms added at the dedupe layer.\n\nChen: 我们把 Redis 客户端切到 pipeline，并启用连接池上限。另外，给 Kafka producer 打开 idempotence 和 acks=all，降低下游重复。链路追踪上我会加一段 span attribute 标记 retry-attempt。\n\nPriya: Rollback or fix-forward?\n\nChen: 先 partial rollback 关闭 transport-level retries。然后 hotfix 加幂等键。灰度发布 5% → 25% → 100%，每步看 error budget burn rate。回头写个 RFC 规范重试策略和幂等要求。\n\n\nDialogue 2: Design review for conflict resolution in a leaderless cache\n\nLin: We’re proposing moving the session cache to a leaderless quorum model. Reads at R=2, writes at W=2 on N=3. Thoughts?\n\nChen: Dynamo-style quorum is fine, but conflict resolution needs clarity. 我们要定义清楚冲突如何解决：LWW 还是基于向量时钟？如果 session 有并发更新，LWW 可能丢字段。\n\nPriya: We can attach a logical clock per field?\n\nChen: 字段级冲突用 CRDT 更稳，比如为计数器用 G-Counter，为集合用 OR-Set。对于非可并行合并的字段，才用 LWW 并记录 causal metadata 以便审计。\n\nDiego: What about write latencies across regions?\n\nChen: 跨区延迟太高的场景下，我们可以做本地写入并异步跨区对账。R+W > N 保证强读，但是会牺牲延迟。我们可以让热路径用 R=1 W=2，冷路径做 read-repair。\n\nLin: Consistent hashing with vnodes?\n\nChen: 必须用 vnodes，目标每节点至少 256 vnodes 便于再平衡。并且将 sessionId 做 SipHash，避免键倾斜。再加 gossip 协议传播 membership 变化。成员变更时，先 warm up cache 再开始接流，减少 cold miss。\n\nPriya: Any anti-entropy plan?\n\nChen: Merkle trees 做段级校验，每 10 分钟跑一次轻量 sync。后台 full sync 每天低峰期跑。另外加一个 compaction-aware throttle，避免 compaction 与 repair 打架。最后，SLA 用 p99<20ms，RTO 15 分钟，写在 SLO 文档里。\n\n\nDialogue 3: Tail latency on gRPC due to head-of-line blocking\n\nDiego: p99 jumped from 60 ms to 180 ms after we consolidated to a single gRPC channel per pod.\n\nChen: 这就是 head-of-line blocking。单连接配合 HTTP/2 流量控制在高 RTT 下会卡。我们要用 subchannel sharding。每个后端维持 4–8 条连接，启用 round_robin，而不是 pick_first。\n\nPriya: Client config supports that?\n\nChen: 支持。并且调大 maxConcurrentStreams，调小 per-stream flow control window，防止单流独占。还要打开 keepalive pings，避免 NAT 超时。服务器端把 maxConnectionAgeJitter 打开，平滑滚动重连。\n\nLin: Any TCP-level tuning?\n\nChen: 可以试试 BBR v2 在 Linux 5.19+。否则用 CUBIC + pacing。关闭 Nagle（TCP_NODELAY）对低延迟小包有利，但注意增大 write buffer，避免过多系统调用。跨区流量要 pin 到同区，通过 xDS locality-weighted LB。\n\nDiego: GC pauses show up too—Go service spikes 40 ms.\n\nChen: 打开 GOGC=100→150，看看内存换延迟。热点路径尽量避免逃逸，改用 sync.Pool。Profiling 显示 slice 扩容？预分配。必要时把 JSON 换成 protobuf，减少反射和分配。\n\nPriya: How do we validate?\n\nChen: 分两阶段实验：lab 里 tc 注入 20ms RTT 验证连接并行度收益；prod 里 5% 金丝雀，启用 adaptive concurrency control，对比 p99、queue depth、retry rate。稳定后扩大到 50%。\n\n\nDialogue 4: Online resharding from 64 to 256 shards\n\nPriya: We need to move from 64 to 256 shards without downtime. Plan?\n\nChen: 在线扩容要避免双写冲突。我的建议是使用一致性哈希环引入空 vnode，然后逐步迁移 key 范围。先实现读双查、写双写。读路径优先新分片，miss 再查旧分片，确保命中率。\n\nLin: How do we backfill?\n\nChen: 用 CDC 流复制。先做静态 backfill，记录高水位点。然后从高水位点开始消费变更日志，追平延迟。延迟 < 5s 时切只读流量到新分片。对账工具做行级校验和。完成后切写流量。最后下线旧分片。\n\nDiego: What about hotspots during move?\n\nChen: 迁移节流很关键。我们按 keyspace 分桶，控制每秒迁移条数，并避开高峰。对大键做拆分或者先迁小键，最后迁大键。加入负载反馈：当目标 shard CPU 或 p99 升高时自动降速。\n\nPriya: Failure handling?\n\nChen: 失败自动重试具备幂等性，迁移记录要有 resume token。出现不可恢复错误时暂停波次，回滚路由表。所有路由变更通过 feature flag，配合灰度。观测方面，暴露 per-shard QPS、lag、error rate、migrate ETA。\n\nLin: Cutover steps?\n\nChen: 先 10% 流量切到新环观察 30 分钟，再到 50%、100%。每步检查 error budget burn rate、写放大、延迟回归。完成后把旧环的数据保留 24 小时只读，防止黑天鹅回滚需要。最后归档指标和运行手册更新。",
    "source": "LLM",
    "generation_strategy": [
      "Persona-based",
      "Chain of Thought"
    ],
    "model_name": "GPT-4",
    "prompt_text": "You are a senior bilingual engineer (Chen) from Beijing working on distributed systems in Seattle. Write English-Mandarin code-switched dialogues reflecting real engineering reasoning. Chen is precise and professional but mixes Mandarin naturally in work chats. Pattern: lexical-borrowing + inter-sentential."
  },
  {
    "id": 52,
    "utterance": "1)\nA: I'll push 新配置 after lunch.\nB: 好的，别忘了 to run 回归测试.\n\n2)\nA: We need to review 合同 before 5.\nB: 行，我会 to highlight 风险点.\n\n3)\nA: Can you test 新功能 on staging?\nB: 没问题，我先 to mock 数据.\n\n4)\nA: I'm going to book 机票 tonight.\nB: 好，记得 to choose 直飞.\n\n5)\nA: Let's clean 历史数据 this sprint.\nB: 可以，我来 to write 清理脚本.\n\n6)\nA: I'll order 午饭 for the team.\nB: 太好了，帮我 to add 素食选项.\n\n7)\nA: We should audit 访问权限 monthly.\nB: 同意，我会 to document 变更.\n\n8)\nA: I'm about to start 面试.\nB: 别忘了 to ask 项目细节.\n\n9)\nA: Let's refactor 用户服务 this week.\nB: 可以，但先 to fix 崩溃日志.\n\n10)\nA: I'll schedule 体检 next month.\nB: 好，记得 to bring 身份证.\n\n11)\nA: We should back up 数据 before deploy.\nB: 是的，我来 to verify 恢复流程.\n\n12)\nA: I'm going to cancel 旧订阅 today.\nB: 行，记得 to export 发票记录.",
    "source": "LLM",
    "generation_strategy": [
      "Few-shot",
      "Chain-of-Thought"
    ],
    "model_name": "GPT-4",
    "prompt_text": "Generate English-Mandarin code-switched dialogues showing micro-level switching patterns. Few-shot examples:\nTemplate 1 (verb-boundary):\nA: \"I'm going to deploy 新分支 right now.\"\nB: \"好，记得 to bump version number.\"\nTemplate 2 (idiom + switch):\nA: \"That fix is a 临时方案 but it works.\"\nB: \"是的，不过我们得做 proper refactor.\"\nTemplate 3 (phrasal-trigger):\nA: \"If cache misses, call fallback.\"\nB: \"如果这样, we'll log event 并且 alert team.\"\nPattern: verb-boundary."
  },
  {
    "id": 53,
    "utterance": "A: 这次优化只是杯水车薪, we need a deeper redesign.\nB: 同意，不过别操之过急, let's gather data first.\n\nA: We had to bite the bullet, 然后把老代码全部重写.\nB: 嗯，既然破釜沉舟了, let's make sure tests are solid.\n\nA: 这需求临阵磨枪也行, but it's risky.\nB: 对，不过别画蛇添足, keep it minimal.\n\nA: Don’t move the goalposts, 我们已经按约定交付了.\nB: 我明白，不过既来之则安之, let's handle the change.\n\nA: 这个PR锦上添花, but not a priority.\nB: 是的，可别因小失大, focus on the outage.\n\nA: We’re walking on thin ice, 所以回滚更稳妥.\nB: 没错，与其病急乱投医, let's do a clean rollback.\n\nA: 这次要稳扎稳打, then we scale.\nB: 别指望一劳永逸, maintenance will continue.\n\nA: It was a can of worms, 接下来先写复盘.\nB: 对，打草惊蛇可不好, so keep comms quiet.\n\nA: 这次优化事半功倍, but monitor closely.\nB: 嗯，别因噎废食, we'll roll out gradually.\n\nA: We hit the nail on the head, 接下来就收尾.\nB: 是啊，夜长梦多, let's ship today.",
    "source": "LLM",
    "generation_strategy": [
      "Few-shot",
      "Chain-of-Thought"
    ],
    "model_name": "GPT-4",
    "prompt_text": "Generate English-Mandarin code-switched dialogues showing micro-level switching patterns. Few-shot examples:\nTemplate 1 (verb-boundary):\nA: \"I'm going to deploy 新分支 right now.\"\nB: \"好，记得 to bump version number.\"\nTemplate 2 (idiom + switch):\nA: \"That fix is a 临时方案 but it works.\"\nB: \"是的，不过我们得做 proper refactor.\"\nTemplate 3 (phrasal-trigger):\nA: \"If cache misses, call fallback.\"\nB: \"如果这样, we'll log event 并且 alert team.\"\nPattern: idiom + switch."
  },
  {
    "id": 54,
    "utterance": "A: When tests fail, 重新跑 pipeline 然后 tag the flaky ones.\nB: 如果是网络问题, we'll reroute traffic 并且 monitor latency.\n\nA: As soon as cache warms, 开启 prefetch 然后 drop the safeguard.\nB: 只要数据稳定, we can relax throttling 并且 remove extra logs.\n\nA: If CI is green, 就 merge 这个 branch 然后 squash commits.\nB: 一旦检测到冲突, resolve locally 然后 push --force-with-lease.\n\nA: When traffic spikes, 先扩容 instances 然后 throttle low-priority jobs.\nB: 除非用户自己修复, don't auto-close tickets 而是先 escalate.\n\nA: If the demo breaks, 就切成 slides 然后 narrate the flow.\nB: 只要观众还在, keep Q&A going 并且 collect follow-ups.\n\nA: Once the backup completes, 验证 snapshot 然后 offload 到 cold storage.\nB: 如果清理失败, we'll pause writes 并且手动 run compaction.\n\nA: When alerts keep firing, 先 mute noisy rules 然后 tune thresholds.\nB: 如果找到根因, document fix 并且 update runbook.\n\nA: If latency spikes, 打开 tracing 然后 sample more requests.\nB: 只要链路明确, we’ll prune hops 并且 cache the hot path.\n\nA: When tests are flaky, 隔离 unstable suites 然后 rerun with retries.\nB: 如果依赖不稳定, pin versions 并且 lock the environment.\n\nA: Once design approves, 就切到 staging 并且 run exploratory tests.\nB: 一旦正向指标提升, widen rollout 然后 notify stakeholders.",
    "source": "LLM",
    "generation_strategy": [
      "Few-shot",
      "Chain-of-Thought"
    ],
    "model_name": "GPT-4",
    "prompt_text": "Generate English-Mandarin code-switched dialogues showing micro-level switching patterns. Few-shot examples:\nTemplate 1 (verb-boundary):\nA: \"I'm going to deploy 新分支 right now.\"\nB: \"好，记得 to bump version number.\"\nTemplate 2 (idiom + switch):\nA: \"That fix is a 临时方案 but it works.\"\nB: \"是的，不过我们得做 proper refactor.\"\nTemplate 3 (phrasal-trigger):\nA: \"If cache misses, call fallback.\"\nB: \"如果这样, we'll log event 并且 alert team.\"\nPattern: phrasal-trigger."
  }
]
